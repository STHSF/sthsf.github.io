<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>personal blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sthsf.github.io/"/>
  <updated>2017-09-01T03:37:45.000Z</updated>
  <id>http://sthsf.github.io/</id>
  
  <author>
    <name>Yu Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Tensorflow基础知识---Bidirectional_RNN</title>
    <link href="http://sthsf.github.io/2017/08/31/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-bidirectional-rnn/"/>
    <id>http://sthsf.github.io/2017/08/31/Tensorflow基础知识-bidirectional-rnn/</id>
    <published>2017-08-31T02:17:27.000Z</published>
    <updated>2017-09-01T03:37:45.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>最近在做一些自然语言处理demo的时候遇到了双向RNN，里面的bidirectional_dynamic_rnn还是值得理解下的，故记录下自己的学习心得。</p>
<h1 id="双向RNNs"><a href="#双向RNNs" class="headerlink" title="双向RNNs"></a>双向RNNs</h1><p>双向RNNs模型是RNN的扩展模型，RNN模型在处理序列模型的学习上主要是依靠上文的信息，双向RNNs模型认为模型的输出不仅仅依靠序列前面的元素，后面的元素对输出也有影响。比如说，想要预测序列中的一个缺失值，我们不仅仅要考虑该缺失值前面的元素，而且要考虑他后面的元素。</p>
<p>简单点来将两个RNN堆叠在一起，分别从两个方向计算序列的output和state，而最终的输出则根据两个RNNs的隐藏状态计算，如下图所示。</p>
<center><img src="RNN-bidirectional.png" height="300" width="500"></center>
<center>Figure 1: A bidirectional RNN model</center>

<p>在每一个时间节点$(x_t)$，这个网络有两层神经元，一层从左向右传播，另一层从右向左传播。为了保证任何时刻t都有两层隐层，这个网络需要消耗两倍的存储量来存储权重和偏置等参数。最终的分类结果是由两层RNN隐层组合来产生最终的结果。</p>
<p>公式1和2表示双向RNN隐层的数学含义。在这两个关系中唯一不同点是循环的方向不一样。公式3展示了通过总结过去和未来词的表示，使用类别的关系来预测下一个词预测。 </p>
<center><img src="compute_formula.png" height="300" width="400"></center>

<p>双向循环神经网络的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络，而且这两个都连接着一个输出层，这个结构提供给输出层输入序列中每个点的完整的过去和未来的上下文信息。</p>
<p>下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层(w1,w3)，隐含层到隐含层自己(w2, w5),向前和向后隐含层到输出层(w4, w6)。</p>
<p>值得注意的是：向后和向前隐含层之间没有信息流，这保证了展开图是非循环的。</p>
<center><img src="bidirectionl-RNN-unrolling.png" height="300" width="500"></center>
<center>Figure 2: Bidirectional RNN model unrolling</center>

<h1 id="Tensorflow中实现双向RNNs"><a href="#Tensorflow中实现双向RNNs" class="headerlink" title="Tensorflow中实现双向RNNs"></a>Tensorflow中实现双向RNNs</h1><p>在tensorflow中已经提供了双向RNNs的接口，使用tf.contrib.rnn.bidirectional_dynamic_rnn()这个函数，就可以很方便的实现双向RNN。</p>
<p>首先看下接口的一些参数<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">bidirectional_dynamic_rnn(</div><div class="line">    cell_fw, # 前向 rnn cell</div><div class="line">    cell_bw, # 反向 rnn cell</div><div class="line">    inputs, # 输入序列.</div><div class="line">    <span class="attribute">sequence_length</span>=None, # 输入序列的实际长度（可选，默认为输入序列的最大长度）</div><div class="line">    <span class="attribute">initial_state_fw</span>=None, # 前向rnn_cell的初始状态（可选）</div><div class="line">    <span class="attribute">initial_state_bw</span>=None, # 反向rnn_cell的初始状态（可选）</div><div class="line">    <span class="attribute">dtype</span>=None, # 初始化和输出的数据类型（可选）</div><div class="line">    <span class="attribute">parallel_iterations</span>=None,</div><div class="line">    <span class="attribute">swap_memory</span>=<span class="literal">False</span>,</div><div class="line">    <span class="attribute">time_major</span>=<span class="literal">False</span>,  # 决定了输入输出tensor的格式：如果为<span class="literal">true</span>, 向量的形状必须为 `[max_time, batch_size, depth]`. </div><div class="line">                       # 如果为<span class="literal">false</span>, tensor的形状必须为`[batch_size, max_time, depth]`. 与dynamic_rnn中的time_major类似。</div><div class="line">    <span class="attribute">scope</span>=None</div><div class="line">)</div></pre></td></tr></table></figure></p>
<p>函数的返回值：<br>一个（outputs, outputs_state）的一个元祖。</p>
<p>其中，</p>
<ul>
<li>outputs=(outputs_fw, outputs_bw),是一个包含前向cell输出tensor和后向tensor输出tensor组成的元祖。</li>
</ul>
<p>若time_major=false，则两个tensor的shape为[batch_size, max_time, depth]，应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。</p>
<p>最终的outputs需要使用tf.concat(outputs, 2)将两者合并起来。</p>
<ul>
<li>outputs_state = (outputs_state_fw， output_state_bw),包含了前向和后向最后的隐藏状态的组成的元祖。outputs_state_fw和output_state_bw的类型都是LSTMStateTuple。LSTMStateTuple由(c, h)组成，分别代表memory cell和hidden state</li>
</ul>
<p>cell_fw和cell_bw的定义是完全一样的，如果两个cell都定义成LSTM就变成说了双向LSTM了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line"><span class="comment"># 正向传播的rnn_cell</span></div><div class="line">cell_fw_lstm = tf.nn.rnn_cell.LSTMCell(<span class="string">'num_units'</span>)</div><div class="line"><span class="comment"># 反向传播的rnn_cell</span></div><div class="line">cell_bw_lstm = tf.nn.rnn_cell.LSTMCell(<span class="string">'num_units'</span>)</div></pre></td></tr></table></figure>
<p>在bidirectional_dynamic_rnn函数内部，会通过array_ops.reverse_sequence函数将输入序列逆序排列，使其达到反向传播的效果。</p>
<p>在实现的时候，我们只需要将定义好的两个cell作为参数传入就可以了：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(outputs, outputs_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw_lstm, cell_bw_lstm, inputs_embedded)</div><div class="line"><span class="meta">#</span><span class="bash"> inputs_embedded为输入的tensor，[batch_szie, max_time, depth]。batch_size为模型当中batch的大小.</span></div><div class="line"><span class="meta">#</span><span class="bash"> 应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。</span></div></pre></td></tr></table></figure></p>
<p>最终的输出outputs = tf.concat((outputs_fw, outputs_bw), 2)或者直接是outputs = tf.concat(outputs, 2)</p>
<p>如果还需要用到最后的输出状态，则需要对（outputs_state_fw， output_state_bw）处理:<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">final_state_c = tf.concat((outputs_state_fw<span class="selector-class">.c</span>, outputs_state_bw.c), <span class="number">1</span>)</div><div class="line">final_state_h = tf.concat((outputs_state_fw<span class="selector-class">.h</span>, outputs_state_bw.h), <span class="number">1</span>)</div><div class="line">outputs_final_state = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.LSTMStateTuple</span>(c=final_state_c,</div><div class="line">                                                    h=final_state_h)</div></pre></td></tr></table></figure></p>
<h3 id="双向LSRTM的实现过如下："><a href="#双向LSRTM的实现过如下：" class="headerlink" title="双向LSRTM的实现过如下："></a>双向LSRTM的实现过如下：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line">vocab_size = <span class="number">1000</span></div><div class="line">embedding_size = <span class="number">50</span></div><div class="line">batch_size =<span class="number">100</span></div><div class="line">max_time = <span class="number">10</span></div><div class="line">hidden_units = <span class="number">10</span></div><div class="line"></div><div class="line">inputs = tf.placeholder(shape=(batch_size, max_time), dtype=tf.int32, name=<span class="string">'inputs'</span>)</div><div class="line">embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>), dtype=tf.float32)</div><div class="line">inputs_embeded = tf.nn.embedding_lookup(embedding, inputs)</div><div class="line"></div><div class="line">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)</div><div class="line"></div><div class="line">((outputs_fw, outputs_bw), (outputs_state_fw, outputs_state_bw)) = tf.nn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, inputs_embeded, sequence_length=max_time)</div><div class="line"></div><div class="line">outputs = tf.concat((outputs_fw, outputs_bw), <span class="number">2</span>)</div><div class="line"></div><div class="line">final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), <span class="number">1</span>)</div><div class="line">final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), <span class="number">1</span>)</div><div class="line">outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c,</div><div class="line">                                                    h=final_state_h)</div></pre></td></tr></table></figure>
<h2 id="多层双向RNNs"><a href="#多层双向RNNs" class="headerlink" title="多层双向RNNs"></a>多层双向RNNs</h2><p>图三展示了一个从较低层传播到下层的多层双向RNN。如图所示，在网络结构中，第t个时间里每一个中间神经元接受到前一个时间（同样的RNN层）传递过来的一组参数，以及之前RNN层传递过来的两组参数。这两组参数一个是从左到右的RNN输入，另一个是从右到左的RNN输入。</p>
<center><img src="multi_bidirectional_rnns.png" height="300" width="400"></center>
<center>Figure 2: Multi-Bidirectional RNN model</center>

<p>为了构建一个L层的RNN，上述的关系将会参照公式4和公式5所修改，其中每一个中间神经元（第i层）的输入是RNN网络中同样的t时刻第i-1层的输出。其中输出，在每一个时刻值为通过所有隐层的输入参数传播的结果（如公式6所示）。</p>
<center><img src="compute_formula2.png" height="300" width="400"></center>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h1&gt;&lt;p&gt;最近在做一些自然语言处理demo的时候遇到了双向RNN，里面的bidirectional_dynamic_rnn还是值得理解
    
    </summary>
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="基础知识" scheme="http://sthsf.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>极大似然估计学习总结</title>
    <link href="http://sthsf.github.io/2017/08/24/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>http://sthsf.github.io/2017/08/24/极大似然估计学习总结/</id>
    <published>2017-08-24T02:09:46.000Z</published>
    <updated>2017-08-24T02:32:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这也样本分布的参数，把可能性最大的那个参数作为真实参数估计。</p>
<h1 id="极大似然估计的原理"><a href="#极大似然估计的原理" class="headerlink" title="极大似然估计的原理"></a>极大似然估计的原理</h1><p>给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="http://blog.csdn.net/sunlylorn/article/details/19610589" target="_blank" rel="external">似然函数</a><br><a href="https://www.zhihu.com/question/54082000" target="_blank" rel="external">如何理解似然函数?</a><br><a href="http://www.cnblogs.com/zhsuiy/p/4822020.html" target="_blank" rel="external">对似然函数的理解</a><br><a href="http://blog.csdn.net/yanqingan/article/details/6125812" target="_blank" rel="external">最大似然估计总结笔记</a><br><a href="http://wiki.mbalib.com/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="external">最大似然估计</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h1&gt;&lt;p&gt;极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这
    
    </summary>
    
      <category term="统计学习" scheme="http://sthsf.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="统计学习" scheme="http://sthsf.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率论" scheme="http://sthsf.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists.</title>
    <link href="http://sthsf.github.io/2017/06/18/ValueError:%20kernel%20already%20exists/"/>
    <id>http://sthsf.github.io/2017/06/18/ValueError: kernel already exists/</id>
    <published>2017-06-18T02:00:00.000Z</published>
    <updated>2017-08-31T05:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>最近在学习使用tensorflow构建language model，遇到关于模型重用的问题，我将模型的训练和预测放在同一个文件中时出现的问题,提示lstm_cell kernal已经存在.</p>
<h1 id="错误提示"><a href="#错误提示" class="headerlink" title="错误提示"></a>错误提示</h1><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">48 Traceback (most recent call last): </div><div class="line">49 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 274, <span class="keyword">in</span> &lt;module&gt; </div><div class="line">50 samp = generate_samples(checkpoint, 20000, prime=<span class="string">"The "</span>) </div><div class="line">51 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 234, <span class="keyword">in</span> generate_samples </div><div class="line">52 <span class="keyword">conf</span>.lstm_size, <span class="keyword">conf</span>.keep_prob, <span class="keyword">conf</span>.grad_clip, False) </div><div class="line">53 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 74, <span class="keyword">in</span> __init__ </div><div class="line">54 self.add_lstm_cell() </div><div class="line">55 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 110, <span class="keyword">in</span> add_lstm_cell </div><div class="line">56 initial_state=self.initial_state) </div><div class="line">57 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 574, ii <span class="keyword">n</span> dynamic_rnn </div><div class="line">58 dtype=dtype) </div><div class="line">59 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 737, ii <span class="keyword">n</span> _dynamic_rnn_loop </div><div class="line">60 swap_memory=swap_memory) </div><div class="line">61 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2770, <span class="keyword">in</span> while_loop </div><div class="line">62 result = context.BuildLoop(cond, body, loop_vars, shape_invariants) </div><div class="line">63 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2599, <span class="keyword">in</span> BuildLoop </div><div class="line">64 pred, body, original_loop_vars, loop_vars, shape_invariants) </div><div class="line">65 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2549, <span class="keyword">in</span> _BuildLoop </div><div class="line">66 body_result = body(*packed_vars_for_body) </div><div class="line">67 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 722, ii <span class="keyword">n</span> _time_step </div><div class="line">68 (output, new_state) = call_cell() </div><div class="line">69 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 708, ii <span class="keyword">n</span> &lt;lambda&gt; </div><div class="line">70 call_cell = lambda: cell(input_t, state) </div><div class="line">71 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, ll ine 180, <span class="keyword">in</span> __call__ </div><div class="line">72 <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state) </div><div class="line">73 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"</span>, <span class="keyword">line</span> 444 1, <span class="keyword">in</span> __call__ </div><div class="line">74 outputs = self.call(inputs, *<span class="keyword">args</span>, **kwargs) </div><div class="line">75 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 916, <span class="keyword">in</span> call </div><div class="line">76 cur_inp, new_state = cell(cur_inp, cur_state) </div><div class="line">77 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 752, <span class="keyword">in</span> __call__ </div><div class="line">78 output, new_state = self._cell(inputs, state, scope) </div><div class="line">79 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 180, <span class="keyword">in</span> __call__ </div><div class="line">80 <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state) </div><div class="line">81 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"</span>, <span class="keyword">line</span> 44 1, <span class="keyword">in</span> __call__ </div><div class="line">82 outputs = self.call(inputs, *<span class="keyword">args</span>, **kwargs) </div><div class="line">83 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 383, <span class="keyword">in</span> call </div><div class="line">84 concat = _linear([inputs, <span class="keyword">h</span>], 4 * self._num_units, True) </div><div class="line">85 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 1017, <span class="keyword">in</span> _linear </div><div class="line">86 initializer=kernel_initializer) </div><div class="line">87 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 1065, <span class="keyword">in</span> get_variable </div><div class="line">88 use_resource=use_resource, custom_getter=custom_getter) </div><div class="line">89 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 962, <span class="keyword">in</span> get_variable </div><div class="line">90 use_resource=use_resource, custom_getter=custom_getter) </div><div class="line">91 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 360, <span class="keyword">in</span> get_variable </div><div class="line">92 validate_shape=validate_shape, use_resource=use_resource) </div><div class="line">93 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 1405, <span class="keyword">in</span> wrapped_custom_getter </div><div class="line">94 *<span class="keyword">args</span>, **kwargs) </div><div class="line">95 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 183, <span class="keyword">in</span> _rnn_get_variable </div><div class="line">96 variable = getter(*<span class="keyword">args</span>, **kwargs) </div><div class="line">97 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 183, <span class="keyword">in</span> _rnn_get_variable </div><div class="line">98 variable = getter(*<span class="keyword">args</span>, **kwargs) </div><div class="line">99 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 352, <span class="keyword">in</span> _true_getter </div><div class="line">100 use_resource=use_resource) </div><div class="line">101 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 664, <span class="keyword">in</span> _get_single_variable </div><div class="line">102 name, <span class="string">""</span>.join(traceback.format_list(tb)))) </div><div class="line">103 ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you <span class="keyword">mean</span> to <span class="keyword">set</span> reuse=True <span class="keyword">in</span> VarScope? Originally defined at:</div><div class="line">104 105 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 110, <span class="keyword">in</span> add_lstm_cell </div><div class="line">106 initial_state=self.initial_state) </div><div class="line">107 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 74, <span class="keyword">in</span> __init__ </div><div class="line">108 self.add_lstm_cell() </div><div class="line">109 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 172, <span class="keyword">in</span> train 110 <span class="keyword">conf</span>.grad_clip, is_training=True)</div></pre></td></tr></table></figure>
<h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>这个问题困扰了我两天，始终找不到解决方案，当我的训练模型和预测模型分开运行时程序没有报错，但是两个程序放在一起运行时就会出现问题，网上搜索的结果大都是关于<a href="https://stackoverflow.com/questions/43957967/tensorflow-v1-1-0-multi-rnn-basiclstmcell-error-reuse-parameter-python-3-5" target="_blank" rel="external">共享权重的问题</a>，错误提示是:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ValueError: Variable hello/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean <span class="keyword">to</span> <span class="builtin-name">set</span> <span class="attribute">reuse</span>=<span class="literal">True</span> <span class="keyword">in</span> VarScope?</div></pre></td></tr></table></figure>
<p>这个error跟我的错误还是有一定的区别的。这些问题主要原因在于使用多层lstm_cell或者双向lstm的时候忽略了定义变量的variable_scope，导致lstm_cell的作用域不一样，但是程序加载的时候并不知道，所以当声明的cell不是同一个的时候，需要用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(name):</div></pre></td></tr></table></figure>
<p>来定义不同的作用范围就可以了，具体还要根据实际情况。</p>
<p>而我的问题好像网上还没有这样的解释，我仔细看错误的提示，分析我的代码，当train和predict放在一起的时候，会调用两次class language_model：这时候就会出现系统里应该存在两个不同的lstm_cell模型，但是系统无法辨别出来，所以会提示<strong>kernel already exists</strong>，而不是<strong>weights already exists</strong>。</p>
<p>而tensorflow有一个reset_default_graph()函数，我对python多线程不是很清楚，贴下源码，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_default_graph</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""Clears the default graph stack and resets the global default graph.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  NOTE: The default graph is a property of the current thread. This</span></div><div class="line"><span class="string">  function applies only to the current thread.  Calling this function while</span></div><div class="line"><span class="string">  a `tf.Session` or `tf.InteractiveSession` is active will result in undefined</span></div><div class="line"><span class="string">  behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects</span></div><div class="line"><span class="string">  after calling this function will result in undefined behavior.</span></div><div class="line"><span class="string">  """</span></div></pre></td></tr></table></figure>
<p>然后在我定义的language_model类中添加这个函数之后之前的问题就解决了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">language_model</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, batch_size=<span class="number">100</span>, seq_length=<span class="number">50</span>, learning_rate=<span class="number">0.01</span>, num_layers=<span class="number">5</span>, hidden_units=<span class="number">128</span>,</span></span></div><div class="line"><span class="function"><span class="params">                 keep_prob=<span class="number">0.8</span>, grad_clip=<span class="number">5</span>, is_training=True)</span>:</span></div><div class="line">        <span class="comment"># 模型的训练和预测放在同一个文件下时如果没有这个函数会报错。</span></div><div class="line">        tf.reset_default_graph()  </div><div class="line">        self.learning_rate = learning_rate</div><div class="line">        self.num_layers = num_layers</div><div class="line">        self.hidden_units = hidden_units</div><div class="line">        self.is_training = is_training</div><div class="line">        self.keep_prob = keep_prob</div><div class="line">        self.grad_clip = grad_clip</div><div class="line">        self.num_classes = num_classes</div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.is_training:</div><div class="line">            self.batch_size = batch_size</div><div class="line">            self.seq_length = seq_length</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.batch_size = <span class="number">1</span></div><div class="line">            self.seq_length = <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'add_input_layer'</span>):</div><div class="line">            self.add_input_layer()</div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'lstm_cell'</span>):</div><div class="line">            self.add_multi_cells()</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'build_output'</span>):</div><div class="line">            self.build_output()</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</div><div class="line">            self.compute_cost()</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'train_op'</span>):</div><div class="line">            self.train_op()</div></pre></td></tr></table></figure>
<p><strong>题外话</strong> ：tensorflow1.2版本之后，定义多层lstm(<code>MultiRNNCell</code>)与原来的版本改变比较大，可以看考<a href="https://www.tensorflow.org/tutorials/recurrent#recurrent-neural-networks" target="_blank" rel="external">PTB tutorials—-Stacking multiple LSTMs</a>.</p>
<p><strong><em>文中涉及到的代码见：<a href="https://github.com/STHSF/DeepNaturalLanguageProcessing/tree/master/language_model/anna" target="_blank" rel="external">github—anna</a></em></strong></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="http://blog.csdn.net/u014283248/article/details/64440268" target="_blank" rel="external">1、tensorflow1.x版本rnn生成cell 报错解决方案</a></p>
<p><a href="http://www.cnblogs.com/max-hu/p/7101164.html" target="_blank" rel="external">2、ValueError: Attempt to reuse RNNCell</a></p>
<p><a href="https://stackoverflow.com/questions/43935609/how-to-reuse-weights-in-multirnncell" target="_blank" rel="external">3、How to reuse weights in MultiRNNCell?</a></p>
<p><a href="https://github.com/tensorflow/tensorflow/issues/8191" target="_blank" rel="external">4、ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h1&gt;&lt;p&gt;最近在学习使用tensorflow构建language model，遇到关于模型重用的问题，我将模型的训练和预测放在同一个文
    
    </summary>
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Deeplearning" scheme="http://sthsf.github.io/tags/Deeplearning/"/>
    
      <category term="ValueError" scheme="http://sthsf.github.io/tags/ValueError/"/>
    
  </entry>
  
  <entry>
    <title>Build personal blog with hexo</title>
    <link href="http://sthsf.github.io/2017/03/18/Build%20personal%20blog%20with%20hexo/"/>
    <id>http://sthsf.github.io/2017/03/18/Build personal blog with hexo/</id>
    <published>2017-03-18T02:51:24.000Z</published>
    <updated>2017-08-22T13:38:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
      <category term="Hexo" scheme="http://sthsf.github.io/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://sthsf.github.io/tags/Hexo/"/>
    
      <category term="Blog" scheme="http://sthsf.github.io/tags/Blog/"/>
    
  </entry>
  
</feed>
