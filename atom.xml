<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yu Li&#39;s personal blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sthsf.github.io/"/>
  <updated>2019-09-19T00:22:32.625Z</updated>
  <id>http://sthsf.github.io/</id>
  
  <author>
    <name>Yu Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python与协程</title>
    <link href="http://sthsf.github.io/2019/08/24/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86--%E5%8D%8F%E7%A8%8B/"/>
    <id>http://sthsf.github.io/2019/08/24/python基础知识--协程/</id>
    <published>2019-08-24T02:09:46.000Z</published>
    <updated>2019-09-19T00:22:32.625Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>在执行一些IO密集型任务的时候, 程序往往会因为等待IO而阻塞. 比如在网络爬虫中, 如果我们使用requests函数库来进行请求的话, 如果网站的响应速度过慢, 程序会一直在等待网站的响应, 最后导致爬虫爬取的效率非常低.</p><p>协程可以用来进行加速, 它对于IO密集型仍无非常有效.</p><h1 id="基本概念"><a class="markdownIt-Anchor" href="#基本概念"></a> 基本概念</h1><p>在了解协程之前, 我们得了解阻塞, 非阻塞, 同步, 异步, 多进程, 协程等基本概念</p><h2 id="1-阻塞"><a class="markdownIt-Anchor" href="#1-阻塞"></a> 1、阻塞</h2><p>阻塞状态是指程序未得到所需计算资源时被挂起的状态. 程序在等待某个操作完成期间, 自身无法继续干别的事情, 则称该程序在该操作上是阻塞的.</p><p>常见的阻塞形式有: 网络I/O阻塞, 磁盘I/O阻塞, 用户输入阻塞等, 阻塞无处不在, 包括CPU切换上下文时, 所有的进程都无法真正干事情, 他们也会被阻塞, 如果是多核CPU, 则正在执行上下文切换操作的核不可被利用.</p><h2 id="2-非阻塞"><a class="markdownIt-Anchor" href="#2-非阻塞"></a> 2、非阻塞</h2><p>程序在等待某些操作过程中, 自身不被阻塞, 可以继续干别的事情, 则称该程序在该操作上是非阻塞的.</p><p>非阻塞并不是在任何程序级别、任何情况下都可以存在, 仅当程序封装的级别可以囊括独立子程序单元时, 它才可能存在非阻塞状态.</p><p>非阻塞的存在是因为阻塞的存在, 正是因为某个操作阻塞导致的耗时与效率地下, 我们才要把它变成非阻塞的.</p><h2 id="3-同步"><a class="markdownIt-Anchor" href="#3-同步"></a> 3、 同步</h2><p>不同程序单元为了完成某个任务, 在执行过程中需要靠某种通信方式以协调一致, 称这些程序单元是同步执行的.</p><p>简言之, 同步意味着有序.</p><h2 id="4-异步"><a class="markdownIt-Anchor" href="#4-异步"></a> 4、异步</h2><p>为了完成某个任务, 不同程序单元之间过程中无需通信协调, 也能完成任务的方式, 不相关的程序单元之间可以是异步的.</p><p>简言之, 异步意味着无序.</p><h2 id="5-多进程"><a class="markdownIt-Anchor" href="#5-多进程"></a> 5、多进程</h2><p>多进程就是利用CPU的多核优势, 在同一时间并行的执行多个任务, 可以大大提高执行效率.</p><h2 id="6-协程"><a class="markdownIt-Anchor" href="#6-协程"></a> 6、协程</h2><p>协程是一种用户态的轻量级线程.</p><p>协程拥有自己的寄存器上下文和栈. 协程调度切换时, 将寄存器上下文和栈保存到其他地方, 在切回来的时候, 恢复先前保存的寄存器上下文和栈, 直接操作栈则基本没有内核切换的开销, 可以不加锁的访问全局变量, 所以上下文切换非常快. 因此协程能保存上次调用时的状态, 即所有局部状态的一个特定组合, 每次过程重入时, 就相当于进入上一次调用的状态.</p><p>协程本质上是个单进程, 协程相对于多进程来说, 无须线程上下文切换的开销, 无需原子操作锁定及同步的开销, 编程模型也非常简单.</p><p>我们可以通过协程来实现异步操作, 不如网络爬虫的场景下, 我们发出一个请求之后, 需要等待一定的时间才能得到响应, 但其实在这个等待的过程中, 程序可以干许多其他的事情, 等到响应得到之后才切换回来继续处理, 这样可以充分利用CPU和其他资源, 这就是异步协程的优势.</p><p>协程的作用是在执行函数A时, 可以随时中断, 去执行另一个函数B, 然后中断继续执行函数A(自由切换). 但这一过程中并不是函数调用(没有调用语句).</p><h2 id="关于同步异步阻塞非阻塞的通俗理解"><a class="markdownIt-Anchor" href="#关于同步异步阻塞非阻塞的通俗理解"></a> 关于同步异步阻塞非阻塞的通俗理解</h2><p>老张爱喝茶，废话不说，煮开水。出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。</p><p>1 老张把水壶放到火上，立等水开。（同步阻塞）老张觉得自己有点傻</p><p>2 老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞）老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。</p><p>3 老张把响水壶放到火上，立等水开。（异步阻塞）老张觉得这样傻等意义不大</p><p>4 老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞）老张觉得自己聪明了。所谓同步异步，只是对于水壶而言。普通水壶，同步；响水壶，异步。虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。所谓阻塞非阻塞，仅仅对于老张而言。立等的老张，阻塞；看电视的老张，非阻塞。情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 并发下载示例</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">(pid, fac)</span>:</span></span><br><span class="line">    response = urllib2.urlopen(<span class="string">'http://json-time.appspot.com/time.json'</span>)</span><br><span class="line">    result = response.read()</span><br><span class="line">    json_result = json.loads(result)</span><br><span class="line">    datetime = json_result[<span class="string">'datetime'</span>]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Process %s %s'</span> % (pid, datetime))</span><br><span class="line">    <span class="keyword">return</span> json_result[<span class="string">'datetime'</span>]</span><br><span class="line"></span><br><span class="line">lists = range(<span class="number">0</span>,<span class="number">10000</span>)</span><br><span class="line">fac = <span class="string">'task name'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">synchronous</span><span class="params">()</span>:</span>  <span class="comment"># 同步一个线程执行函数</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> lists:</span><br><span class="line">        task(i, fac)</span><br><span class="line">    print(time.time() - t)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asynchronous</span><span class="params">()</span>:</span>  <span class="comment"># 异步一个线程执行函数</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    threads = [gevent.spawn(task, i, fac) <span class="keyword">for</span> i <span class="keyword">in</span> lists]   <span class="comment"># gevent.spawn 时，协程已经开始执行</span></span><br><span class="line">    gevent.joinall(threads)  <span class="comment"># gevent.joinall 只是用来等待所有协程执行完毕</span></span><br><span class="line">    print(time.time() - t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># def asynchornoius2():</span></span><br><span class="line"><span class="comment">#     t = time.time()</span></span><br><span class="line"><span class="comment">#     x = Pool(40)</span></span><br><span class="line"><span class="comment">#     for i in lists:</span></span><br><span class="line"><span class="comment">#         x.imap(task)</span></span><br><span class="line"><span class="comment">#     # x.join()</span></span><br><span class="line"><span class="comment">#     print(time.time() - t)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># def asynchornoius3():</span></span><br><span class="line"><span class="comment">#     t = time.time()</span></span><br><span class="line"><span class="comment">#     x = Pool(40)</span></span><br><span class="line"><span class="comment">#     for i in lists:</span></span><br><span class="line"><span class="comment">#         x.imap_unordered(task, i, fac)</span></span><br><span class="line"><span class="comment">#     x.join()</span></span><br><span class="line"><span class="comment">#     print(time.time() - t)</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'synchronous:'</span>)</span><br><span class="line">synchronous()   <span class="comment"># 同步执行时要等待执行完后再执行</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'asynchronous:'</span>)</span><br><span class="line">asynchronous()  <span class="comment"># 异步时遇到等待则会切换执行</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'asynchronius:'</span>)</span><br><span class="line">asynchornoius()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print('-----------')</span></span><br><span class="line"><span class="comment"># asynchornoius2()</span></span><br><span class="line"><span class="comment"># print('-----------')</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># asynchornoius3()</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent.pool <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey; monkey.patch_all()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制并发的 greenlets</span></span><br><span class="line">pool = Pool(<span class="number">300</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asynchornoius</span><span class="params">()</span>:</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    greenlets = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> lists:</span><br><span class="line">        greenlet = pool.spawn(task, i, fac)</span><br><span class="line">        greenlets.append(greenlet)</span><br><span class="line">    pool.join()</span><br><span class="line">    print(time.time() - t)</span><br><span class="line">    <span class="comment"># 协程中避免使用全局变量来进行状态统计，或者结果收集。</span></span><br><span class="line">    <span class="comment"># 若要收集结果，可以使用 value 属性来获取每个 greenlet 的返回值。</span></span><br><span class="line">    <span class="keyword">for</span> greenlet <span class="keyword">in</span> greenlets:</span><br><span class="line">        <span class="keyword">print</span> greenlet.value</span><br></pre></td></tr></table></figure><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="https://cuiqingcai.com/6160.html" target="_blank" rel="noopener">Python中异步协程的使用方法介绍</a><br><a href="http://hhkbp2.com/gevent-tutorial/#monkey-patching" target="_blank" rel="noopener">gevent程序员指南</a><br><a href="https://www.sunzhongwei.com/gevent.html" target="_blank" rel="noopener">gevent Tricks</a><br><a href="https://www.coder4.com/archives/2192" target="_blank" rel="noopener">关于gevent的Timeout(超时)问题……</a><br><a href="https://www.cnblogs.com/piperck/p/8044632.html" target="_blank" rel="noopener">Gevent 性能和 gevent.loop 的运用和带来的思考</a><br><a href="https://blog.csdn.net/feixiaoxing/article/details/79056535" target="_blank" rel="noopener">延时和锁</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="python基础知识" scheme="http://sthsf.github.io/categories/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="python" scheme="http://sthsf.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>智能问答系统的探索和实践</title>
    <link href="http://sthsf.github.io/2019/03/16/math/%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
    <id>http://sthsf.github.io/2019/03/16/math/智能问答系统的探索和实践/</id>
    <published>2019-03-16T02:09:46.000Z</published>
    <updated>2019-09-19T01:08:04.712Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>知识图谱 (Knowledge Graph) 是当前的研究热点。自从2012年Google推出自己第一版知识图谱以来，它在学术界和工业界掀起了一股热潮。各大互联网企业在之后的短短一年内纷纷推出了自己的知识图谱产品以作为回应。那么与这些传统的互联网公司相比，对处于当今风口浪尖上的行业 - 互联网金融， 知识图谱可以有哪方面的应用呢？</p><h1 id="极大似然估计的原理"><a class="markdownIt-Anchor" href="#极大似然估计的原理"></a> 极大似然估计的原理</h1><p>给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。</p><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="https://www.jiqizhixin.com/articles/2018-06-20-4" target="_blank" rel="noopener">这是一份通俗易懂的知识图谱技术与应用指南</a><br><a href="https://www.jiqizhixin.com/articles/2018-12-27-15" target="_blank" rel="noopener">知识图谱在互联网金融行业的应用</a><br><a href="https://www.jiqizhixin.com/articles/2018-06-19" target="_blank" rel="noopener">知识图谱的技术与应用</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="智能问答系统" scheme="http://sthsf.github.io/categories/%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="综述" scheme="http://sthsf.github.io/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="智能问答系统" scheme="http://sthsf.github.io/tags/%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱的基础知识和应用场景</title>
    <link href="http://sthsf.github.io/2019/03/15/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%92%8C%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
    <id>http://sthsf.github.io/2019/03/15/知识图谱的基础知识和应用场景/</id>
    <published>2019-03-15T02:09:46.000Z</published>
    <updated>2019-03-18T05:56:49.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>知识图谱 (Knowledge Graph) 是当前的研究热点。自从2012年Google推出自己第一版知识图谱以来，它在学术界和工业界掀起了一股热潮。各大互联网企业在之后的短短一年内纷纷推出了自己的知识图谱产品以作为回应。那么与这些传统的互联网公司相比，对处于当今风口浪尖上的行业 - 互联网金融， 知识图谱可以有哪方面的应用呢？</p><h1 id="聊天机器人"><a class="markdownIt-Anchor" href="#聊天机器人"></a> 聊天机器人</h1><p><a href="https://mp.weixin.qq.com/s/aHY9peDgWAVDom4at6vtSw" target="_blank" rel="noopener">聊天机器人对知识图谱有哪些特殊的需求？</a><br>给定一个概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit" style="margin-right:.02778em">D</span><span class="mclose">)</span></span></span></span>，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>f</mi><mi>D</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(f_D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.02778em">D</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>,以及一个分布参数，我们可以从这个分布中抽出一个具有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>个值的采样。</p><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="https://www.jiqizhixin.com/articles/2018-06-20-4" target="_blank" rel="noopener">这是一份通俗易懂的知识图谱技术与应用指南</a></p><p><a href="https://www.jiqizhixin.com/articles/2018-12-27-15" target="_blank" rel="noopener">知识图谱在互联网金融行业的应用</a></p><p><a href="https://www.jiqizhixin.com/articles/2018-06-19" target="_blank" rel="noopener">知识图谱的技术与应用</a></p><p><a href="https://github.com/memect/kg-beijing/wiki/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%BB%84%E5%AD%A6%E4%B9%A0%E5%A4%A7%E7%BA%B2" target="_blank" rel="noopener">kg-beijing</a></p><p><a href="https://www.jianshu.com/p/cd937f20bf55" target="_blank" rel="noopener">知识图谱基础</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="知识图谱" scheme="http://sthsf.github.io/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    
      <category term="知识图谱" scheme="http://sthsf.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="综述" scheme="http://sthsf.github.io/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的共享变量机制</title>
    <link href="http://sthsf.github.io/2017/09/14/Tensorflow%E4%B8%AD%E7%9A%84%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E6%9C%BA%E5%88%B6/"/>
    <id>http://sthsf.github.io/2017/09/14/Tensorflow中的共享变量机制/</id>
    <published>2017-09-14T02:14:56.000Z</published>
    <updated>2019-09-17T01:30:35.899Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>本文总结了Tensorflow使用过程中易混淆的一些接口，比如get_variable()和Variable()。name_scope()和variable_scope()等。</p><h2 id="tfget_varibale-tfvariable"><a class="markdownIt-Anchor" href="#tfget_varibale-tfvariable"></a> tf.get_varibale()/ tf.Variable()</h2><p>tensorflow中关于variable的op有tf.get_variable()和tf.Variable两个.</p><h3 id="tfget_variable"><a class="markdownIt-Anchor" href="#tfget_variable"></a> tf.get_variable()</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(name,</span><br><span class="line">                <span class="attribute">shape</span>=None,</span><br><span class="line">                <span class="attribute">dtype</span>=None,</span><br><span class="line">                <span class="attribute">initializer</span>=None,</span><br><span class="line">                <span class="attribute">regularizer</span>=None,</span><br><span class="line">                <span class="attribute">trainable</span>=<span class="literal">True</span>,</span><br><span class="line">                <span class="attribute">collections</span>=None,</span><br><span class="line">                <span class="attribute">caching_device</span>=None,</span><br><span class="line">                <span class="attribute">partitioner</span>=None,</span><br><span class="line">                <span class="attribute">validate_shape</span>=<span class="literal">True</span>,</span><br><span class="line">                <span class="attribute">use_resource</span>=None,</span><br><span class="line">                <span class="attribute">custom_getter</span>=None)</span><br></pre></td></tr></table></figure><h3 id="tfvariable"><a class="markdownIt-Anchor" href="#tfvariable"></a> tf.Variable()</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(<span class="attribute">initial_value</span>=None,</span><br><span class="line">            <span class="attribute">trainable</span>=<span class="literal">True</span>,</span><br><span class="line">            <span class="attribute">collections</span>=None,</span><br><span class="line">            <span class="attribute">validate_shape</span>=<span class="literal">True</span>,</span><br><span class="line">            <span class="attribute">caching_device</span>=None,</span><br><span class="line">            <span class="attribute">name</span>=None,</span><br><span class="line">            <span class="attribute">variable_def</span>=None,</span><br><span class="line">            <span class="attribute">dtype</span>=None,</span><br><span class="line">            <span class="attribute">expected_shape</span>=None,</span><br><span class="line">            <span class="attribute">import_scope</span>=None)</span><br></pre></td></tr></table></figure><h3 id="tfget_variabletfvariable的区别"><a class="markdownIt-Anchor" href="#tfget_variabletfvariable的区别"></a> tf.get_variable()/tf.Variable()的区别</h3><p>先看下面的两个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line">a1 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a1"</span>)</span><br><span class="line">a2 = tf.Variable(<span class="number">1</span>, name=<span class="string">'a1'</span>)</span><br><span class="line">print(a1.name)</span><br><span class="line">print(a2.name)</span><br></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">a1:</span><span class="number">0</span></span><br><span class="line"><span class="symbol">a1_1:</span><span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a1 = tf.get_variable(<span class="string">"a1"</span>, <span class="number">0</span>)</span><br><span class="line">a2 = tf.get_variable(<span class="string">'a1'</span>, <span class="number">0</span>)</span><br><span class="line">print(a1.name)</span><br><span class="line">print(a2.name)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: Variable a1 already exists, disallowed. Did you mean <span class="keyword">to</span> <span class="builtin-name">set</span> <span class="attribute">reuse</span>=<span class="literal">True</span> <span class="keyword">in</span> VarScope?</span><br></pre></td></tr></table></figure><p><strong>使用tf.Varibale()定义变量的时候，如果检测到命名冲突，系统会自动解决，但是使用tf.get_varibale()时，系统不会解决冲突，并且会报错</strong>。<br>所以如果需要<strong>共享变量</strong>则需要使用tf.get_variable()。在其他情况下两者的用法基本一样。</p><p>我们再来看一段代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>):</span><br><span class="line">    w1 = tf.get_variable(<span class="string">"w1"</span>, shape=[])</span><br><span class="line">    w2 = tf.Variable(<span class="number">0.0</span>, name=<span class="string">"w2"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"scope1"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    w1_p = tf.get_variable(<span class="string">"w1"</span>, shape=[])</span><br><span class="line">    w2_p = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"w2"</span>) </span><br><span class="line"></span><br><span class="line">print(w1 <span class="keyword">is</span> w1_p, w2 <span class="keyword">is</span> w2_p)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight hy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name">True</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>从输出结果可以看出，对于get_variable()，来说，如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的。<br>而tf.Variable()每次都在创建新对象。<br>这里没有太多的提到共享变量的问题，</p><h2 id="tfname_scope-tfvariable_scope"><a class="markdownIt-Anchor" href="#tfname_scope-tfvariable_scope"></a> tf.name_scope()/ tf.variable_scope()</h2><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="http://blog.csdn.net/u012436149/article/details/53696970" target="_blank" rel="noopener">tensorflow学习笔记（二十三）：variable与get_variable</a><br><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/variable_scope.html" target="_blank" rel="noopener">共享变量</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/categories/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/tags/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中dynamic_rnn和row_rnn的区别</title>
    <link href="http://sthsf.github.io/2017/09/04/Tensorflow%E4%B8%ADdynamic-rnn%E5%92%8Crow-rnn%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://sthsf.github.io/2017/09/04/Tensorflow中dynamic-rnn和row-rnn的区别/</id>
    <published>2017-09-04T00:58:38.000Z</published>
    <updated>2017-09-14T01:28:17.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="https://www.zhihu.com/question/52200883" target="_blank" rel="noopener">tensor flow dynamic_rnn 与rnn有啥区别？</a><br><a href="https://www.zhihu.com/question/61311860" target="_blank" rel="noopener">有大神能详细讲讲tensorflow中的raw_rnn这个函数么？</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/categories/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/tags/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow基础知识---Bidirectional_RNN</title>
    <link href="http://sthsf.github.io/2017/08/31/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-bidirectional-rnn/"/>
    <id>http://sthsf.github.io/2017/08/31/Tensorflow基础知识-bidirectional-rnn/</id>
    <published>2017-08-31T02:17:27.000Z</published>
    <updated>2017-09-14T02:22:59.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>最近在做一些自然语言处理demo的时候遇到了双向RNN，里面的bidirectional_dynamic_rnn和static_bidirectional_rnn还是值得理解下的，故记录下自己的学习心得。</p><h1 id="双向rnns"><a class="markdownIt-Anchor" href="#双向rnns"></a> 双向RNNs</h1><p>双向RNNs模型是RNN的扩展模型，RNN模型在处理序列模型的学习上主要是依靠上文的信息，双向RNNs模型认为模型的输出不仅仅依靠序列前面的元素，后面的元素对输出也有影响。比如说，想要预测序列中的一个缺失值，我们不仅仅要考虑该缺失值前面的元素，而且要考虑他后面的元素。</p><p>简单点来将两个RNN堆叠在一起，分别从两个方向计算序列的output和state，而最终的输出则根据两个RNNs的隐藏状态计算，如下图所示。</p><center><img src="RNN-bidirectional.png" height="300" width="400"></center><center>Figure 1: A bidirectional RNN model</center><p>在每一个时间节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>，这个网络有两层神经元，一层从左向右传播，另一层从右向左传播。为了保证任何时刻t都有两层隐层，这个网络需要消耗两倍的存储量来存储权重和偏置等参数。最终的分类结果是由两层RNN隐层组合来产生最终的结果。</p><p>公式1和2表示双向RNN隐层的数学含义。在这两个关系中唯一不同点是循环的方向不一样。公式3展示了通过总结过去和未来词的表示，使用类别的关系来预测下一个词预测。</p><center><img src="compute_formula.png" height="300" width="400"></center><p>双向循环神经网络的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络，而且这两个都连接着一个输出层，这个结构提供给输出层输入序列中每个点的完整的过去和未来的上下文信息。</p><p>下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层(w1,w3)，隐含层到隐含层自己(w2, w5),向前和向后隐含层到输出层(w4, w6)。</p><p>值得注意的是：向后和向前隐含层之间没有信息流，是独立计算的，只是最后输出的时候把二者的状态向量结合起来，这保证了展开图是非循环的。</p><center><img src="bidirectionl-RNN-unrolling.png" height="300" width="400"></center><center>Figure 2: Bidirectional RNN model unrolling</center><h1 id="tensorflow中实现双向rnns"><a class="markdownIt-Anchor" href="#tensorflow中实现双向rnns"></a> Tensorflow中实现双向RNNs</h1><h2 id="tfcontribrnnbidirectional_dynamic_rnn"><a class="markdownIt-Anchor" href="#tfcontribrnnbidirectional_dynamic_rnn"></a> tf.contrib.rnn.bidirectional_dynamic_rnn()</h2><p>在tensorflow中已经提供了双向RNNs的接口，使用tf.contrib.rnn.bidirectional_dynamic_rnn()这个函数，就可以很方便的构建双向RNN网络。</p><p>首先看下接口的一些参数</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bidirectional_dynamic_rnn(</span><br><span class="line">    cell_fw, # 前向 rnn cell</span><br><span class="line">    cell_bw, # 反向 rnn cell</span><br><span class="line">    inputs, # 输入序列.</span><br><span class="line">    <span class="attribute">sequence_length</span>=None, # 输入序列的实际长度（可选，默认为输入序列的最大长度）</span><br><span class="line">    <span class="attribute">initial_state_fw</span>=None, # 前向rnn_cell的初始状态（可选）</span><br><span class="line">    <span class="attribute">initial_state_bw</span>=None, # 反向rnn_cell的初始状态（可选）</span><br><span class="line">    <span class="attribute">dtype</span>=None, # 初始化和输出的数据类型（可选）</span><br><span class="line">    <span class="attribute">parallel_iterations</span>=None,</span><br><span class="line">    <span class="attribute">swap_memory</span>=<span class="literal">False</span>,</span><br><span class="line">    <span class="attribute">time_major</span>=<span class="literal">False</span>,  # 决定了输入输出tensor的格式：如果为<span class="literal">true</span>, 向量的形状必须为 `[max_time, batch_size, depth]`. </span><br><span class="line">                       # 如果为<span class="literal">false</span>, tensor的形状必须为`[batch_size, max_time, depth]`. 与dynamic_rnn中的time_major类似。</span><br><span class="line">    <span class="attribute">scope</span>=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>函数的返回值：<br>一个（outputs, outputs_state）的一个元祖。</p><p>其中，</p><ul><li>outputs=(outputs_fw, outputs_bw),是一个包含前向cell输出tensor和后向tensor输出tensor组成的元祖。</li></ul><p>若time_major=false，则两个tensor的shape为[batch_size, max_time, depth]，应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。</p><p>最终的outputs需要使用tf.concat(outputs, 2)将两者合并起来。</p><ul><li>outputs_state = (outputs_state_fw， output_state_bw),包含了前向和后向最后的隐藏状态的组成的元祖。outputs_state_fw和output_state_bw的类型都是LSTMStateTuple。LSTMStateTuple由(c, h)组成，分别代表memory cell和hidden state</li></ul><p>cell_fw和cell_bw的定义是完全一样的，如果两个cell都定义成LSTM就变成说了双向LSTM了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="comment"># 正向传播的rnn_cell单元，这里使用的是LSRTMCell</span></span><br><span class="line">cell_fw_lstm = tf.nn.rnn_cell.LSTMCell(<span class="string">'num_units'</span>)</span><br><span class="line"><span class="comment"># 反向传播的rnn_cell单元，与正向传播的rnn单元相同</span></span><br><span class="line">cell_bw_lstm = tf.nn.rnn_cell.LSTMCell(<span class="string">'num_units'</span>)</span><br></pre></td></tr></table></figure><p>在bidirectional_dynamic_rnn函数内部，会通过array_ops.reverse_sequence函数将输入序列逆序排列，使其达到反向传播的效果。</p><p>在实现的时候，我们只需要将定义好的两个cell作为参数传入就可以了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(outputs, outputs_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw_lstm, cell_bw_lstm, inputs_embedded)</span><br><span class="line"><span class="meta">#</span><span class="bash"> inputs_embedded为输入的tensor，[batch_szie, max_time, depth]。batch_size为模型当中batch的大小.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。</span></span><br></pre></td></tr></table></figure><p>最终的输出outputs = tf.concat((outputs_fw, outputs_bw), 2)或者直接是outputs = tf.concat(outputs, 2)</p><p>如果还需要用到最后的输出状态，则需要对（outputs_state_fw， output_state_bw）处理:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">final_state_c = tf.concat((outputs_state_fw<span class="selector-class">.c</span>, outputs_state_bw.c), <span class="number">1</span>)</span><br><span class="line">final_state_h = tf.concat((outputs_state_fw<span class="selector-class">.h</span>, outputs_state_bw.h), <span class="number">1</span>)</span><br><span class="line">outputs_final_state = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.LSTMStateTuple</span>(c=final_state_c,</span><br><span class="line">                                                    h=final_state_h)</span><br></pre></td></tr></table></figure><h3 id="双向lsrtm的实现过如下"><a class="markdownIt-Anchor" href="#双向lsrtm的实现过如下"></a> 双向LSRTM的实现过如下：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line">vocab_size = <span class="number">1000</span></span><br><span class="line">embedding_size = <span class="number">50</span></span><br><span class="line">batch_size =<span class="number">100</span></span><br><span class="line">max_time = <span class="number">10</span></span><br><span class="line">hidden_units = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">inputs = tf.placeholder(shape=(batch_size, max_time), dtype=tf.int32, name=<span class="string">'inputs'</span>)</span><br><span class="line">embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>), dtype=tf.float32)</span><br><span class="line">inputs_embeded = tf.nn.embedding_lookup(embedding, inputs)</span><br><span class="line"></span><br><span class="line">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)</span><br><span class="line"></span><br><span class="line">((outputs_fw, outputs_bw), (outputs_state_fw, outputs_state_bw)) = tf.nn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, inputs_embeded, sequence_length=max_time)</span><br><span class="line"></span><br><span class="line">outputs = tf.concat((outputs_fw, outputs_bw), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), <span class="number">1</span>)</span><br><span class="line">final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), <span class="number">1</span>)</span><br><span class="line">outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c,</span><br><span class="line">                                                    h=final_state_h)</span><br></pre></td></tr></table></figure><h2 id="tfcontribrnnstatic_bidirectional_rnn"><a class="markdownIt-Anchor" href="#tfcontribrnnstatic_bidirectional_rnn"></a> tf.contrib.rnn.static_bidirectional_rnn()</h2><h1 id="多层双向rnns"><a class="markdownIt-Anchor" href="#多层双向rnns"></a> 多层双向RNNs</h1><p>图三展示了一个从较低层传播到下层的多层双向RNN。如图所示，在网络结构中，第t个时间里每一个中间神经元接受到前一个时间（同样的RNN层）传递过来的一组参数，以及之前RNN层传递过来的两组参数。这两组参数一个是从左到右的RNN输入，另一个是从右到左的RNN输入。</p><center><img src="multi_bidirectional_rnns.png" height="300" width="400"></center><center>Figure 3: Multi-Bidirectional RNN model</center><p>为了构建一个L层的RNN，上述的关系将会参照公式4和公式5所修改，其中每一个中间神经元（第i层）的输入是RNN网络中同样的t时刻第i-1层的输出。其中输出，在每一个时刻值为通过所有隐层的输入参数传播的结果（如公式6所示）。</p><center><img src="compute_formula2.png" height="300" width="400"></center><h2 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h2><p><a href="http://blog.csdn.net/wuzqChom/article/details/75453327" target="_blank" rel="noopener">tensorflow.nn.bidirectional_dynamic_rnn()函数的用法</a></p><p><a href="https://www.zybuluo.com/hanxiaoyang/note/438990" target="_blank" rel="noopener">深度学习与自然语言处理(7)_斯坦福cs224d 语言模型，RNN，LSTM与GRU</a></p><p><a href="http://blog.csdn.net/u012436149/article/details/71080601" target="_blank" rel="noopener">tensorflow学习笔记(三十九):双向rnn</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/categories/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/tags/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>极大似然估计学习总结</title>
    <link href="http://sthsf.github.io/2017/08/24/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>http://sthsf.github.io/2017/08/24/极大似然估计学习总结/</id>
    <published>2017-08-24T02:09:46.000Z</published>
    <updated>2019-03-14T06:51:45.027Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这也样本分布的参数，把可能性最大的那个参数作为真实参数估计。</p><h1 id="极大似然估计的原理"><a class="markdownIt-Anchor" href="#极大似然估计的原理"></a> 极大似然估计的原理</h1><p>给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。</p><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="http://blog.csdn.net/sunlylorn/article/details/19610589" target="_blank" rel="noopener">似然函数</a><br><a href="https://www.zhihu.com/question/54082000" target="_blank" rel="noopener">如何理解似然函数?</a><br><a href="http://www.cnblogs.com/zhsuiy/p/4822020.html" target="_blank" rel="noopener">对似然函数的理解</a><br><a href="http://blog.csdn.net/yanqingan/article/details/6125812" target="_blank" rel="noopener">最大似然估计总结笔记</a><br><a href="http://wiki.mbalib.com/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="noopener">最大似然估计</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="统计学习" scheme="http://sthsf.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="统计学习" scheme="http://sthsf.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率论" scheme="http://sthsf.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists.</title>
    <link href="http://sthsf.github.io/2017/06/18/ValueError:%20kernel%20already%20exists/"/>
    <id>http://sthsf.github.io/2017/06/18/ValueError: kernel already exists/</id>
    <published>2017-06-18T02:00:00.000Z</published>
    <updated>2017-08-31T05:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>最近在学习使用tensorflow构建language model，遇到关于模型重用的问题，我将模型的训练和预测放在同一个文件中时出现的问题,提示lstm_cell kernal已经存在.</p><h1 id="错误提示"><a class="markdownIt-Anchor" href="#错误提示"></a> 错误提示</h1><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">48 Traceback (most recent call last): </span><br><span class="line">49 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 274, <span class="keyword">in</span> &lt;module&gt; </span><br><span class="line">50 samp = generate_samples(checkpoint, 20000, prime=<span class="string">"The "</span>) </span><br><span class="line">51 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 234, <span class="keyword">in</span> generate_samples </span><br><span class="line">52 <span class="keyword">conf</span>.lstm_size, <span class="keyword">conf</span>.keep_prob, <span class="keyword">conf</span>.grad_clip, False) </span><br><span class="line">53 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 74, <span class="keyword">in</span> __init__ </span><br><span class="line">54 self.add_lstm_cell() </span><br><span class="line">55 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 110, <span class="keyword">in</span> add_lstm_cell </span><br><span class="line">56 initial_state=self.initial_state) </span><br><span class="line">57 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 574, ii <span class="keyword">n</span> dynamic_rnn </span><br><span class="line">58 dtype=dtype) </span><br><span class="line">59 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 737, ii <span class="keyword">n</span> _dynamic_rnn_loop </span><br><span class="line">60 swap_memory=swap_memory) </span><br><span class="line">61 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2770, <span class="keyword">in</span> while_loop </span><br><span class="line">62 result = context.BuildLoop(cond, body, loop_vars, shape_invariants) </span><br><span class="line">63 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2599, <span class="keyword">in</span> BuildLoop </span><br><span class="line">64 pred, body, original_loop_vars, loop_vars, shape_invariants) </span><br><span class="line">65 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2549, <span class="keyword">in</span> _BuildLoop </span><br><span class="line">66 body_result = body(*packed_vars_for_body) </span><br><span class="line">67 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 722, ii <span class="keyword">n</span> _time_step </span><br><span class="line">68 (output, new_state) = call_cell() </span><br><span class="line">69 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 708, ii <span class="keyword">n</span> &lt;lambda&gt; </span><br><span class="line">70 call_cell = lambda: cell(input_t, state) </span><br><span class="line">71 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, ll ine 180, <span class="keyword">in</span> __call__ </span><br><span class="line">72 <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state) </span><br><span class="line">73 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"</span>, <span class="keyword">line</span> 444 1, <span class="keyword">in</span> __call__ </span><br><span class="line">74 outputs = self.call(inputs, *<span class="keyword">args</span>, **kwargs) </span><br><span class="line">75 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 916, <span class="keyword">in</span> call </span><br><span class="line">76 cur_inp, new_state = cell(cur_inp, cur_state) </span><br><span class="line">77 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 752, <span class="keyword">in</span> __call__ </span><br><span class="line">78 output, new_state = self._cell(inputs, state, scope) </span><br><span class="line">79 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 180, <span class="keyword">in</span> __call__ </span><br><span class="line">80 <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state) </span><br><span class="line">81 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"</span>, <span class="keyword">line</span> 44 1, <span class="keyword">in</span> __call__ </span><br><span class="line">82 outputs = self.call(inputs, *<span class="keyword">args</span>, **kwargs) </span><br><span class="line">83 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 383, <span class="keyword">in</span> call </span><br><span class="line">84 concat = _linear([inputs, <span class="keyword">h</span>], 4 * self._num_units, True) </span><br><span class="line">85 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 1017, <span class="keyword">in</span> _linear </span><br><span class="line">86 initializer=kernel_initializer) </span><br><span class="line">87 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 1065, <span class="keyword">in</span> get_variable </span><br><span class="line">88 use_resource=use_resource, custom_getter=custom_getter) </span><br><span class="line">89 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 962, <span class="keyword">in</span> get_variable </span><br><span class="line">90 use_resource=use_resource, custom_getter=custom_getter) </span><br><span class="line">91 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 360, <span class="keyword">in</span> get_variable </span><br><span class="line">92 validate_shape=validate_shape, use_resource=use_resource) </span><br><span class="line">93 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 1405, <span class="keyword">in</span> wrapped_custom_getter </span><br><span class="line">94 *<span class="keyword">args</span>, **kwargs) </span><br><span class="line">95 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 183, <span class="keyword">in</span> _rnn_get_variable </span><br><span class="line">96 variable = getter(*<span class="keyword">args</span>, **kwargs) </span><br><span class="line">97 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 183, <span class="keyword">in</span> _rnn_get_variable </span><br><span class="line">98 variable = getter(*<span class="keyword">args</span>, **kwargs) </span><br><span class="line">99 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 352, <span class="keyword">in</span> _true_getter </span><br><span class="line">100 use_resource=use_resource) </span><br><span class="line">101 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 664, <span class="keyword">in</span> _get_single_variable </span><br><span class="line">102 name, <span class="string">""</span>.join(traceback.format_list(tb)))) </span><br><span class="line">103 ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you <span class="keyword">mean</span> to <span class="keyword">set</span> reuse=True <span class="keyword">in</span> VarScope? Originally defined at:</span><br><span class="line">104 105 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 110, <span class="keyword">in</span> add_lstm_cell </span><br><span class="line">106 initial_state=self.initial_state) </span><br><span class="line">107 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 74, <span class="keyword">in</span> __init__ </span><br><span class="line">108 self.add_lstm_cell() </span><br><span class="line">109 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 172, <span class="keyword">in</span> train 110 <span class="keyword">conf</span>.grad_clip, is_training=True)</span><br></pre></td></tr></table></figure><h1 id="解决方法"><a class="markdownIt-Anchor" href="#解决方法"></a> 解决方法</h1><p>这个问题困扰了我两天，始终找不到解决方案，当我的训练模型和预测模型分开运行时程序没有报错，但是两个程序放在一起运行时就会出现问题，网上搜索的结果大都是关于<a href="https://stackoverflow.com/questions/43957967/tensorflow-v1-1-0-multi-rnn-basiclstmcell-error-reuse-parameter-python-3-5" target="_blank" rel="noopener">共享权重的问题</a>，错误提示是:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: Variable hello/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean <span class="keyword">to</span> <span class="builtin-name">set</span> <span class="attribute">reuse</span>=<span class="literal">True</span> <span class="keyword">in</span> VarScope?</span><br></pre></td></tr></table></figure><p>这个error跟我的错误还是有一定的区别的。这些问题主要原因在于使用多层lstm_cell或者双向lstm的时候忽略了定义变量的variable_scope，导致lstm_cell的作用域不一样，但是程序加载的时候并不知道，所以当声明的cell不是同一个的时候，需要用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(name):</span><br></pre></td></tr></table></figure><p>来定义不同的作用范围就可以了，具体还要根据实际情况。</p><p>而我的问题好像网上还没有这样的解释，我仔细看错误的提示，分析我的代码，当train和predict放在一起的时候，会调用两次class language_model：这时候就会出现系统里应该存在两个不同的lstm_cell模型，但是系统无法辨别出来，所以会提示<strong>kernel already exists</strong>，而不是<strong>weights already exists</strong>。</p><p>而tensorflow有一个reset_default_graph()函数，我对python多线程不是很清楚，贴下源码，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_default_graph</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">"""Clears the default graph stack and resets the global default graph.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  NOTE: The default graph is a property of the current thread. This</span></span><br><span class="line"><span class="string">  function applies only to the current thread.  Calling this function while</span></span><br><span class="line"><span class="string">  a `tf.Session` or `tf.InteractiveSession` is active will result in undefined</span></span><br><span class="line"><span class="string">  behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects</span></span><br><span class="line"><span class="string">  after calling this function will result in undefined behavior.</span></span><br><span class="line"><span class="string">  """</span></span><br></pre></td></tr></table></figure><p>然后在我定义的language_model类中添加这个函数之后之前的问题就解决了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">language_model</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, batch_size=<span class="number">100</span>, seq_length=<span class="number">50</span>, learning_rate=<span class="number">0.01</span>, num_layers=<span class="number">5</span>, hidden_units=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 keep_prob=<span class="number">0.8</span>, grad_clip=<span class="number">5</span>, is_training=True)</span>:</span></span><br><span class="line">        <span class="comment"># 模型的训练和预测放在同一个文件下时如果没有这个函数会报错。</span></span><br><span class="line">        tf.reset_default_graph()  </span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line">        self.is_training = is_training</span><br><span class="line">        self.keep_prob = keep_prob</span><br><span class="line">        self.grad_clip = grad_clip</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_training:</span><br><span class="line">            self.batch_size = batch_size</span><br><span class="line">            self.seq_length = seq_length</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batch_size = <span class="number">1</span></span><br><span class="line">            self.seq_length = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'add_input_layer'</span>):</span><br><span class="line">            self.add_input_layer()</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'lstm_cell'</span>):</span><br><span class="line">            self.add_multi_cells()</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'build_output'</span>):</span><br><span class="line">            self.build_output()</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</span><br><span class="line">            self.compute_cost()</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'train_op'</span>):</span><br><span class="line">            self.train_op()</span><br></pre></td></tr></table></figure><p><strong>题外话</strong> ：tensorflow1.2版本之后，定义多层lstm(<code>MultiRNNCell</code>)与原来的版本改变比较大，可以看考<a href="https://www.tensorflow.org/tutorials/recurrent#recurrent-neural-networks" target="_blank" rel="noopener">PTB tutorials—Stacking multiple LSTMs</a>.</p><p><em><strong>文中涉及到的代码见：<a href="https://github.com/STHSF/DeepNaturalLanguageProcessing/tree/master/language_model/anna" target="_blank" rel="noopener">github–anna</a></strong></em></p><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="http://blog.csdn.net/u014283248/article/details/64440268" target="_blank" rel="noopener">1、tensorflow1.x版本rnn生成cell 报错解决方案</a></p><p><a href="http://www.cnblogs.com/max-hu/p/7101164.html" target="_blank" rel="noopener">2、ValueError: Attempt to reuse RNNCell</a></p><p><a href="https://stackoverflow.com/questions/43935609/how-to-reuse-weights-in-multirnncell" target="_blank" rel="noopener">3、How to reuse weights in MultiRNNCell?</a></p><p><a href="https://github.com/tensorflow/tensorflow/issues/8191" target="_blank" rel="noopener">4、ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面
      
    
    </summary>
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Deeplearning" scheme="http://sthsf.github.io/tags/Deeplearning/"/>
    
      <category term="ValueError" scheme="http://sthsf.github.io/tags/ValueError/"/>
    
  </entry>
  
  <entry>
    <title>Errors when buildding blog with hexo</title>
    <link href="http://sthsf.github.io/2017/03/18/Errors%20when%20buildding%20blog%20with%20hexo/"/>
    <id>http://sthsf.github.io/2017/03/18/Errors when buildding blog with hexo/</id>
    <published>2017-03-18T02:51:24.000Z</published>
    <updated>2017-09-04T02:33:52.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --><h2 id="hexo-安装提交过程中错误以及解决方法"><a class="markdownIt-Anchor" href="#hexo-安装提交过程中错误以及解决方法"></a> Hexo 安装提交过程中错误以及解决方法</h2><h4 id="1-错误提示"><a class="markdownIt-Anchor" href="#1-错误提示"></a> 1、错误提示：</h4><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR Process failed: _posts/Tensorflow中dynamic-rnn和row-rnn的区别.md</span><br><span class="line">YAMLException: can not read a block mapping <span class="built_in">entry</span>; a multiline key may not be an <span class="keyword">implicit</span> key at line <span class="number">9</span>, column <span class="number">1</span>:</span><br></pre></td></tr></table></figure><p>错误原因：原因是我在makrdown文件中的site那块添加categories的时候，后面的冒号使用的是中文状态下的输入。<br>解决方法：中文冒号改写成英文冒号</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">Tensorflow中dynamic_rnn和row_rnn的区别</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2017</span><span class="bullet">-09</span><span class="bullet">-04</span> <span class="number">08</span><span class="string">:58:38</span></span><br><span class="line"><span class="attr">catalog:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Tensorflow</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Tensorflow基础知识</span></span><br><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Tensorflow基础知识</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p><strong>注意:注意中英文标点符号的问题，还有就是拼写错误，冒号后面的空格等等格式错误，有的时候不注意很容易写错。</strong></p><h4 id="2-错误提示"><a class="markdownIt-Anchor" href="#2-错误提示"></a> 2、错误提示</h4><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">fatal</span>: unable to access <span class="string">'https://github.com/STHSF/sthsf.github.io/'</span>: Could not resolve <span class="attribute">host</span>: github.com</span><br><span class="line">FATAL Something's wrong. Maybe you can find the solution <span class="attribute">here</span>: <span class="attribute">http</span>:<span class="comment">//hexo.io/docs/troubleshooting.html</span></span><br></pre></td></tr></table></figure><p>错误原因：可能是网络原因，导致无法连接到github的服务器。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Sep 19 2019 09:08:14 GMT+0800 (China Standard Time) --&gt;&lt;h2 id=&quot;hexo-安装提交过程中错误以及解决方法&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;
      
    
    </summary>
    
      <category term="Hexo" scheme="http://sthsf.github.io/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://sthsf.github.io/tags/Hexo/"/>
    
      <category term="Blog" scheme="http://sthsf.github.io/tags/Blog/"/>
    
  </entry>
  
</feed>
