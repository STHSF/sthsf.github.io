<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yu Li&#39;s personal blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sthsf.github.io/"/>
  <updated>2017-09-06T03:48:07.000Z</updated>
  <id>http://sthsf.github.io/</id>
  
  <author>
    <name>Yu Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>hello</title>
    <link href="http://sthsf.github.io/2017/09/06/hello/"/>
    <id>http://sthsf.github.io/2017/09/06/hello/</id>
    <published>2017-09-06T03:42:29.000Z</published>
    <updated>2017-09-06T03:48:07.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>(</mo><msubsup><mi>σ</mi><mrow><mi>i</mi></mrow><mi>x</mi></msubsup><msubsup><mi>σ</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>x</mi></msubsup><mo>+</mo><mi>g</mi><msubsup><mi>σ</mi><mrow><mi>i</mi></mrow><mi>z</mi></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">H=-\sum_{i=1}^N (\sigma_{i}^x \sigma_{i+1}^x+g \sigma_{i}^z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.08125em">H</span><span class="mrel">=</span><span class="mord">−</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">σ</span><span class="vlist"><span style="top:.247em;margin-left:-.03588em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span style="top:-.4129999999999999em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">x</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">σ</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">x</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">σ</span><span class="vlist"><span style="top:.247em;margin-left:-.03588em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span style="top:-.4129999999999999em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.04398em">z</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>n</mi><mo>)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mrow><mfrac><mrow><mi>n</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">f</mi><mtext></mtext></mtext><mi>n</mi><mtext><mtext></mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mtext></mtext><mi mathvariant="normal">e</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mtext></mrow></mtd></mtr><mtr><mtd><mrow><mn>3</mn><mi>n</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">f</mi><mtext></mtext></mtext><mi>n</mi><mtext><mtext></mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mtext></mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">d</mi></mtext></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">f(n) = \begin{cases} \frac{n}{2}, &amp; \text{if } n\text{ is even} \\ 3n+1, &amp; \text{if } n\text{ is odd} \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.86678em"></span><span class="strut bottom" style="height:3.2335599999999998em;vertical-align:-1.3667799999999999em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mclose">)</span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist"><span style="top:-.75922em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mpunct">,</span></span></span><span style="top:.9347799999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathit">n</span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="mpunct">,</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist"><span style="top:-.75922em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:.07778em">f</span><span class="mord mspace"> </span></span><span class="mord mathit">n</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mspace"> </span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mspace"> </span><span class="mord mathrm">e</span><span class="mord mathrm" style="margin-right:.01389em">v</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span></span></span></span><span style="top:.9347799999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:.07778em">f</span><span class="mord mspace"> </span></span><span class="mord mathit">n</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mspace"> </span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mspace"> </span><span class="mord mathrm">o</span><span class="mord mathrm">d</span><span class="mord mathrm">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --&gt;&lt;p&gt;&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;ma
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Tensorflow中dynamic_rnn和row_rnn的区别</title>
    <link href="http://sthsf.github.io/2017/09/04/Tensorflow%E4%B8%ADdynamic-rnn%E5%92%8Crow-rnn%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://sthsf.github.io/2017/09/04/Tensorflow中dynamic-rnn和row-rnn的区别/</id>
    <published>2017-09-04T00:58:38.000Z</published>
    <updated>2017-09-04T01:36:52.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --><h1 id="xie"><a class="markdownIt-Anchor" href="#xie"></a> xie</h1><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="https://www.zhihu.com/question/52200883" target="_blank" rel="external">tensor flow dynamic_rnn 与rnn有啥区别？</a><br><a href="https://www.zhihu.com/question/61311860" target="_blank" rel="external">有大神能详细讲讲tensorflow中的raw_rnn这个函数么？</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --&gt;&lt;h1 id=&quot;xie&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#xie&quot;&gt;&lt;/a&gt; xie&lt;/h1&gt;&lt;h1 id=&quot;参考文献&quot;&gt;
    
    </summary>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/categories/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/tags/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow基础知识---Bidirectional_RNN</title>
    <link href="http://sthsf.github.io/2017/08/31/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-bidirectional-rnn/"/>
    <id>http://sthsf.github.io/2017/08/31/Tensorflow基础知识-bidirectional-rnn/</id>
    <published>2017-08-31T02:17:27.000Z</published>
    <updated>2017-09-07T01:58:47.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>最近在做一些自然语言处理demo的时候遇到了双向RNN，里面的bidirectional_dynamic_rnn还是值得理解下的，故记录下自己的学习心得。</p><h1 id="双向rnns"><a class="markdownIt-Anchor" href="#双向rnns"></a> 双向RNNs</h1><p>双向RNNs模型是RNN的扩展模型，RNN模型在处理序列模型的学习上主要是依靠上文的信息，双向RNNs模型认为模型的输出不仅仅依靠序列前面的元素，后面的元素对输出也有影响。比如说，想要预测序列中的一个缺失值，我们不仅仅要考虑该缺失值前面的元素，而且要考虑他后面的元素。</p><p>简单点来将两个RNN堆叠在一起，分别从两个方向计算序列的output和state，而最终的输出则根据两个RNNs的隐藏状态计算，如下图所示。</p>\overrightarrow {h}^{(i)}{_t} = f(\overrightarrow {W}^{(i)} h^{(i-1)}_t + \overrightarrow V^{(i)} \overrightarrow h^{(i)}{_{t-1}} + \overrightarrow b^{(i)})\tag{4} \overleftarrow {h}^{(i)}_t = f(\overleftarrow{W}^{i}h^{(i-1)}_t + \overleftarrow{V}^{(i)} \overleftarrow{h}^{(i)}_{t+1} + \overleftarrow b^{(i)})\tag{5} \widehat{y}_t=g( U h_t + c)=g( U[\overrightarrow {h}^{(L)}_t;\overleftarrow{h}^{(L)}_t] + c)\tag{6}<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>(</mo><msubsup><mi>σ</mi><mrow><mi>i</mi></mrow><mi>x</mi></msubsup><msubsup><mi>σ</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>x</mi></msubsup><mo>+</mo><mi>g</mi><msubsup><mi>σ</mi><mrow><mi>i</mi></mrow><mi>z</mi></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">H=-\sum_{i=1}^N (\sigma_{i}^x \sigma_{i+1}^x+g \sigma_{i}^z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.08125em">H</span><span class="mrel">=</span><span class="mord">−</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">σ</span><span class="vlist"><span style="top:.247em;margin-left:-.03588em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span style="top:-.4129999999999999em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">x</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">σ</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">x</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">σ</span><span class="vlist"><span style="top:.247em;margin-left:-.03588em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span style="top:-.4129999999999999em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.04398em">z</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>n</mi><mo>)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mrow><mfrac><mrow><mi>n</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">f</mi><mtext></mtext></mtext><mi>n</mi><mtext><mtext></mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mtext></mtext><mi mathvariant="normal">e</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi></mtext></mrow></mtd></mtr><mtr><mtd><mrow><mn>3</mn><mi>n</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">f</mi><mtext></mtext></mtext><mi>n</mi><mtext><mtext></mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mtext></mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">d</mi></mtext></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">f(n) = \begin{cases} \frac{n}{2}, &amp; \text{if } n\text{ is even} \\ 3n+1, &amp; \text{if } n\text{ is odd} \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.86678em"></span><span class="strut bottom" style="height:3.2335599999999998em;vertical-align:-1.3667799999999999em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mclose">)</span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist"><span style="top:-.75922em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mpunct">,</span></span></span><span style="top:.9347799999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathit">n</span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="mpunct">,</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist"><span style="top:-.75922em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:.07778em">f</span><span class="mord mspace"> </span></span><span class="mord mathit">n</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mspace"> </span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mspace"> </span><span class="mord mathrm">e</span><span class="mord mathrm" style="margin-right:.01389em">v</span><span class="mord mathrm">e</span><span class="mord mathrm">n</span></span></span></span><span style="top:.9347799999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="text mord displaystyle textstyle uncramped"><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:.07778em">f</span><span class="mord mspace"> </span></span><span class="mord mathit">n</span><span class="text mord displaystyle textstyle uncramped"><span class="mord mspace"> </span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mspace"> </span><span class="mord mathrm">o</span><span class="mord mathrm">d</span><span class="mord mathrm">d</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><center><img src="RNN-bidirectional.png" height="300" width="400"></center><center>Figure 1: A bidirectional RNN model</center><p>在每一个时间节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>，这个网络有两层神经元，一层从左向右传播，另一层从右向左传播。为了保证任何时刻t都有两层隐层，这个网络需要消耗两倍的存储量来存储权重和偏置等参数。最终的分类结果是由两层RNN隐层组合来产生最终的结果。</p><p>公式1和2表示双向RNN隐层的数学含义。在这两个关系中唯一不同点是循环的方向不一样。公式3展示了通过总结过去和未来词的表示，使用类别的关系来预测下一个词预测。</p><center><img src="compute_formula.png" height="300" width="400"></center><p>双向循环神经网络的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络，而且这两个都连接着一个输出层，这个结构提供给输出层输入序列中每个点的完整的过去和未来的上下文信息。</p><p>下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层(w1,w3)，隐含层到隐含层自己(w2, w5),向前和向后隐含层到输出层(w4, w6)。</p><p>值得注意的是：向后和向前隐含层之间没有信息流，是独立计算的，只是最后输出的时候把二者的状态向量结合起来，这保证了展开图是非循环的。</p><center><img src="bidirectionl-RNN-unrolling.png" height="300" width="400"></center><center>Figure 2: Bidirectional RNN model unrolling</center><h1 id="tensorflow中实现双向rnns"><a class="markdownIt-Anchor" href="#tensorflow中实现双向rnns"></a> Tensorflow中实现双向RNNs</h1><p>在tensorflow中已经提供了双向RNNs的接口，使用tf.contrib.rnn.bidirectional_dynamic_rnn()这个函数，就可以很方便的实现双向RNN。</p><p>首先看下接口的一些参数</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">bidirectional_dynamic_rnn(</div><div class="line">    cell_fw, # 前向 rnn cell</div><div class="line">    cell_bw, # 反向 rnn cell</div><div class="line">    inputs, # 输入序列.</div><div class="line">    <span class="attribute">sequence_length</span>=None, # 输入序列的实际长度（可选，默认为输入序列的最大长度）</div><div class="line">    <span class="attribute">initial_state_fw</span>=None, # 前向rnn_cell的初始状态（可选）</div><div class="line">    <span class="attribute">initial_state_bw</span>=None, # 反向rnn_cell的初始状态（可选）</div><div class="line">    <span class="attribute">dtype</span>=None, # 初始化和输出的数据类型（可选）</div><div class="line">    <span class="attribute">parallel_iterations</span>=None,</div><div class="line">    <span class="attribute">swap_memory</span>=<span class="literal">False</span>,</div><div class="line">    <span class="attribute">time_major</span>=<span class="literal">False</span>,  # 决定了输入输出tensor的格式：如果为<span class="literal">true</span>, 向量的形状必须为 `[max_time, batch_size, depth]`. </div><div class="line">                       # 如果为<span class="literal">false</span>, tensor的形状必须为`[batch_size, max_time, depth]`. 与dynamic_rnn中的time_major类似。</div><div class="line">    <span class="attribute">scope</span>=None</div><div class="line">)</div></pre></td></tr></table></figure><p>函数的返回值：<br>一个（outputs, outputs_state）的一个元祖。</p><p>其中，</p><ul><li>outputs=(outputs_fw, outputs_bw),是一个包含前向cell输出tensor和后向tensor输出tensor组成的元祖。</li></ul><p>若time_major=false，则两个tensor的shape为[batch_size, max_time, depth]，应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。</p><p>最终的outputs需要使用tf.concat(outputs, 2)将两者合并起来。</p><ul><li>outputs_state = (outputs_state_fw， output_state_bw),包含了前向和后向最后的隐藏状态的组成的元祖。outputs_state_fw和output_state_bw的类型都是LSTMStateTuple。LSTMStateTuple由(c, h)组成，分别代表memory cell和hidden state</li></ul><p>cell_fw和cell_bw的定义是完全一样的，如果两个cell都定义成LSTM就变成说了双向LSTM了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line"><span class="comment"># 正向传播的rnn_cell</span></div><div class="line">cell_fw_lstm = tf.nn.rnn_cell.LSTMCell(<span class="string">'num_units'</span>)</div><div class="line"><span class="comment"># 反向传播的rnn_cell</span></div><div class="line">cell_bw_lstm = tf.nn.rnn_cell.LSTMCell(<span class="string">'num_units'</span>)</div></pre></td></tr></table></figure><p>在bidirectional_dynamic_rnn函数内部，会通过array_ops.reverse_sequence函数将输入序列逆序排列，使其达到反向传播的效果。</p><p>在实现的时候，我们只需要将定义好的两个cell作为参数传入就可以了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(outputs, outputs_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw_lstm, cell_bw_lstm, inputs_embedded)</div><div class="line"><span class="meta">#</span><span class="bash"> inputs_embedded为输入的tensor，[batch_szie, max_time, depth]。batch_size为模型当中batch的大小.</span></div><div class="line"><span class="meta">#</span><span class="bash"> 应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。</span></div></pre></td></tr></table></figure><p>最终的输出outputs = tf.concat((outputs_fw, outputs_bw), 2)或者直接是outputs = tf.concat(outputs, 2)</p><p>如果还需要用到最后的输出状态，则需要对（outputs_state_fw， output_state_bw）处理:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">final_state_c = tf.concat((outputs_state_fw<span class="selector-class">.c</span>, outputs_state_bw.c), <span class="number">1</span>)</div><div class="line">final_state_h = tf.concat((outputs_state_fw<span class="selector-class">.h</span>, outputs_state_bw.h), <span class="number">1</span>)</div><div class="line">outputs_final_state = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.LSTMStateTuple</span>(c=final_state_c,</div><div class="line">                                                    h=final_state_h)</div></pre></td></tr></table></figure><h3 id="双向lsrtm的实现过如下"><a class="markdownIt-Anchor" href="#双向lsrtm的实现过如下"></a> 双向LSRTM的实现过如下：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </div><div class="line">vocab_size = <span class="number">1000</span></div><div class="line">embedding_size = <span class="number">50</span></div><div class="line">batch_size =<span class="number">100</span></div><div class="line">max_time = <span class="number">10</span></div><div class="line">hidden_units = <span class="number">10</span></div><div class="line"></div><div class="line">inputs = tf.placeholder(shape=(batch_size, max_time), dtype=tf.int32, name=<span class="string">'inputs'</span>)</div><div class="line">embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>), dtype=tf.float32)</div><div class="line">inputs_embeded = tf.nn.embedding_lookup(embedding, inputs)</div><div class="line"></div><div class="line">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)</div><div class="line"></div><div class="line">((outputs_fw, outputs_bw), (outputs_state_fw, outputs_state_bw)) = tf.nn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, inputs_embeded, sequence_length=max_time)</div><div class="line"></div><div class="line">outputs = tf.concat((outputs_fw, outputs_bw), <span class="number">2</span>)</div><div class="line"></div><div class="line">final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), <span class="number">1</span>)</div><div class="line">final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), <span class="number">1</span>)</div><div class="line">outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c,</div><div class="line">                                                    h=final_state_h)</div></pre></td></tr></table></figure><h2 id="多层双向rnns"><a class="markdownIt-Anchor" href="#多层双向rnns"></a> 多层双向RNNs</h2><p>图三展示了一个从较低层传播到下层的多层双向RNN。如图所示，在网络结构中，第t个时间里每一个中间神经元接受到前一个时间（同样的RNN层）传递过来的一组参数，以及之前RNN层传递过来的两组参数。这两组参数一个是从左到右的RNN输入，另一个是从右到左的RNN输入。</p><center><img src="multi_bidirectional_rnns.png" height="300" width="400"></center><center>Figure 3: Multi-Bidirectional RNN model</center><p>为了构建一个L层的RNN，上述的关系将会参照公式4和公式5所修改，其中每一个中间神经元（第i层）的输入是RNN网络中同样的t时刻第i-1层的输出。其中输出，在每一个时刻值为通过所有隐层的输入参数传播的结果（如公式6所示）。</p><center><img src="compute_formula2.png" height="300" width="400"></center><h2 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h2><p><a href="http://blog.csdn.net/wuzqChom/article/details/75453327" target="_blank" rel="external">tensorflow.nn.bidirectional_dynamic_rnn()函数的用法</a></p><p><a href="https://www.zybuluo.com/hanxiaoyang/note/438990" target="_blank" rel="external">深度学习与自然语言处理(7)_斯坦福cs224d 语言模型，RNN，LSTM与GRU</a></p><p><a href="http://blog.csdn.net/u012436149/article/details/71080601" target="_blank" rel="external">tensorflow学习笔记(三十九):双向rnn</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面&lt;/h1&gt;&lt;p&gt;最近在做一些自然
    
    </summary>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/categories/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Tensorflow基础知识" scheme="http://sthsf.github.io/tags/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>极大似然估计学习总结</title>
    <link href="http://sthsf.github.io/2017/08/24/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>http://sthsf.github.io/2017/08/24/极大似然估计学习总结/</id>
    <published>2017-08-24T02:09:46.000Z</published>
    <updated>2017-08-24T02:32:27.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这也样本分布的参数，把可能性最大的那个参数作为真实参数估计。</p><h1 id="极大似然估计的原理"><a class="markdownIt-Anchor" href="#极大似然估计的原理"></a> 极大似然估计的原理</h1><p>给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。</p><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="http://blog.csdn.net/sunlylorn/article/details/19610589" target="_blank" rel="external">似然函数</a><br><a href="https://www.zhihu.com/question/54082000" target="_blank" rel="external">如何理解似然函数?</a><br><a href="http://www.cnblogs.com/zhsuiy/p/4822020.html" target="_blank" rel="external">对似然函数的理解</a><br><a href="http://blog.csdn.net/yanqingan/article/details/6125812" target="_blank" rel="external">最大似然估计总结笔记</a><br><a href="http://wiki.mbalib.com/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="external">最大似然估计</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面&lt;/h1&gt;&lt;p&gt;极大似然估计是一
    
    </summary>
    
      <category term="统计学习" scheme="http://sthsf.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="统计学习" scheme="http://sthsf.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率论" scheme="http://sthsf.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists.</title>
    <link href="http://sthsf.github.io/2017/06/18/ValueError:%20kernel%20already%20exists/"/>
    <id>http://sthsf.github.io/2017/06/18/ValueError: kernel already exists/</id>
    <published>2017-06-18T02:00:00.000Z</published>
    <updated>2017-08-31T05:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --><h1 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h1><p>最近在学习使用tensorflow构建language model，遇到关于模型重用的问题，我将模型的训练和预测放在同一个文件中时出现的问题,提示lstm_cell kernal已经存在.</p><h1 id="错误提示"><a class="markdownIt-Anchor" href="#错误提示"></a> 错误提示</h1><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">48 Traceback (most recent call last): </div><div class="line">49 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 274, <span class="keyword">in</span> &lt;module&gt; </div><div class="line">50 samp = generate_samples(checkpoint, 20000, prime=<span class="string">"The "</span>) </div><div class="line">51 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 234, <span class="keyword">in</span> generate_samples </div><div class="line">52 <span class="keyword">conf</span>.lstm_size, <span class="keyword">conf</span>.keep_prob, <span class="keyword">conf</span>.grad_clip, False) </div><div class="line">53 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 74, <span class="keyword">in</span> __init__ </div><div class="line">54 self.add_lstm_cell() </div><div class="line">55 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 110, <span class="keyword">in</span> add_lstm_cell </div><div class="line">56 initial_state=self.initial_state) </div><div class="line">57 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 574, ii <span class="keyword">n</span> dynamic_rnn </div><div class="line">58 dtype=dtype) </div><div class="line">59 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 737, ii <span class="keyword">n</span> _dynamic_rnn_loop </div><div class="line">60 swap_memory=swap_memory) </div><div class="line">61 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2770, <span class="keyword">in</span> while_loop </div><div class="line">62 result = context.BuildLoop(cond, body, loop_vars, shape_invariants) </div><div class="line">63 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2599, <span class="keyword">in</span> BuildLoop </div><div class="line">64 pred, body, original_loop_vars, loop_vars, shape_invariants) </div><div class="line">65 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"</span>" , <span class="keyword">line</span> 2549, <span class="keyword">in</span> _BuildLoop </div><div class="line">66 body_result = body(*packed_vars_for_body) </div><div class="line">67 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 722, ii <span class="keyword">n</span> _time_step </div><div class="line">68 (output, new_state) = call_cell() </div><div class="line">69 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"</span>, <span class="keyword">line</span> 708, ii <span class="keyword">n</span> &lt;lambda&gt; </div><div class="line">70 call_cell = lambda: cell(input_t, state) </div><div class="line">71 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, ll ine 180, <span class="keyword">in</span> __call__ </div><div class="line">72 <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state) </div><div class="line">73 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"</span>, <span class="keyword">line</span> 444 1, <span class="keyword">in</span> __call__ </div><div class="line">74 outputs = self.call(inputs, *<span class="keyword">args</span>, **kwargs) </div><div class="line">75 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 916, <span class="keyword">in</span> call </div><div class="line">76 cur_inp, new_state = cell(cur_inp, cur_state) </div><div class="line">77 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 752, <span class="keyword">in</span> __call__ </div><div class="line">78 output, new_state = self._cell(inputs, state, scope) </div><div class="line">79 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 180, <span class="keyword">in</span> __call__ </div><div class="line">80 <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state) </div><div class="line">81 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"</span>, <span class="keyword">line</span> 44 1, <span class="keyword">in</span> __call__ </div><div class="line">82 outputs = self.call(inputs, *<span class="keyword">args</span>, **kwargs) </div><div class="line">83 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 383, <span class="keyword">in</span> call </div><div class="line">84 concat = _linear([inputs, <span class="keyword">h</span>], 4 * self._num_units, True) </div><div class="line">85 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 1017, <span class="keyword">in</span> _linear </div><div class="line">86 initializer=kernel_initializer) </div><div class="line">87 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 1065, <span class="keyword">in</span> get_variable </div><div class="line">88 use_resource=use_resource, custom_getter=custom_getter) </div><div class="line">89 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 962, <span class="keyword">in</span> get_variable </div><div class="line">90 use_resource=use_resource, custom_getter=custom_getter) </div><div class="line">91 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 360, <span class="keyword">in</span> get_variable </div><div class="line">92 validate_shape=validate_shape, use_resource=use_resource) </div><div class="line">93 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 1405, <span class="keyword">in</span> wrapped_custom_getter </div><div class="line">94 *<span class="keyword">args</span>, **kwargs) </div><div class="line">95 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 183, <span class="keyword">in</span> _rnn_get_variable </div><div class="line">96 variable = getter(*<span class="keyword">args</span>, **kwargs) </div><div class="line">97 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"</span>, <span class="keyword">l</span> ine 183, <span class="keyword">in</span> _rnn_get_variable </div><div class="line">98 variable = getter(*<span class="keyword">args</span>, **kwargs) </div><div class="line">99 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 352, <span class="keyword">in</span> _true_getter </div><div class="line">100 use_resource=use_resource) </div><div class="line">101 <span class="keyword">File</span> <span class="string">"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"</span>, <span class="keyword">line</span> 664, <span class="keyword">in</span> _get_single_variable </div><div class="line">102 name, <span class="string">""</span>.join(traceback.format_list(tb)))) </div><div class="line">103 ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you <span class="keyword">mean</span> to <span class="keyword">set</span> reuse=True <span class="keyword">in</span> VarScope? Originally defined at:</div><div class="line">104 105 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 110, <span class="keyword">in</span> add_lstm_cell </div><div class="line">106 initial_state=self.initial_state) </div><div class="line">107 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 74, <span class="keyword">in</span> __init__ </div><div class="line">108 self.add_lstm_cell() </div><div class="line">109 <span class="keyword">File</span> <span class="string">"anna_writer.py"</span>, <span class="keyword">line</span> 172, <span class="keyword">in</span> train 110 <span class="keyword">conf</span>.grad_clip, is_training=True)</div></pre></td></tr></table></figure><h1 id="解决方法"><a class="markdownIt-Anchor" href="#解决方法"></a> 解决方法</h1><p>这个问题困扰了我两天，始终找不到解决方案，当我的训练模型和预测模型分开运行时程序没有报错，但是两个程序放在一起运行时就会出现问题，网上搜索的结果大都是关于<a href="https://stackoverflow.com/questions/43957967/tensorflow-v1-1-0-multi-rnn-basiclstmcell-error-reuse-parameter-python-3-5" target="_blank" rel="external">共享权重的问题</a>，错误提示是:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ValueError: Variable hello/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean <span class="keyword">to</span> <span class="builtin-name">set</span> <span class="attribute">reuse</span>=<span class="literal">True</span> <span class="keyword">in</span> VarScope?</div></pre></td></tr></table></figure><p>这个error跟我的错误还是有一定的区别的。这些问题主要原因在于使用多层lstm_cell或者双向lstm的时候忽略了定义变量的variable_scope，导致lstm_cell的作用域不一样，但是程序加载的时候并不知道，所以当声明的cell不是同一个的时候，需要用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(name):</div></pre></td></tr></table></figure><p>来定义不同的作用范围就可以了，具体还要根据实际情况。</p><p>而我的问题好像网上还没有这样的解释，我仔细看错误的提示，分析我的代码，当train和predict放在一起的时候，会调用两次class language_model：这时候就会出现系统里应该存在两个不同的lstm_cell模型，但是系统无法辨别出来，所以会提示<strong>kernel already exists</strong>，而不是<strong>weights already exists</strong>。</p><p>而tensorflow有一个reset_default_graph()函数，我对python多线程不是很清楚，贴下源码，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_default_graph</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"""Clears the default graph stack and resets the global default graph.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">  NOTE: The default graph is a property of the current thread. This</span></div><div class="line"><span class="string">  function applies only to the current thread.  Calling this function while</span></div><div class="line"><span class="string">  a `tf.Session` or `tf.InteractiveSession` is active will result in undefined</span></div><div class="line"><span class="string">  behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects</span></div><div class="line"><span class="string">  after calling this function will result in undefined behavior.</span></div><div class="line"><span class="string">  """</span></div></pre></td></tr></table></figure><p>然后在我定义的language_model类中添加这个函数之后之前的问题就解决了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">language_model</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, batch_size=<span class="number">100</span>, seq_length=<span class="number">50</span>, learning_rate=<span class="number">0.01</span>, num_layers=<span class="number">5</span>, hidden_units=<span class="number">128</span>,</span></span></div><div class="line"><span class="function"><span class="params">                 keep_prob=<span class="number">0.8</span>, grad_clip=<span class="number">5</span>, is_training=True)</span>:</span></div><div class="line">        <span class="comment"># 模型的训练和预测放在同一个文件下时如果没有这个函数会报错。</span></div><div class="line">        tf.reset_default_graph()  </div><div class="line">        self.learning_rate = learning_rate</div><div class="line">        self.num_layers = num_layers</div><div class="line">        self.hidden_units = hidden_units</div><div class="line">        self.is_training = is_training</div><div class="line">        self.keep_prob = keep_prob</div><div class="line">        self.grad_clip = grad_clip</div><div class="line">        self.num_classes = num_classes</div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.is_training:</div><div class="line">            self.batch_size = batch_size</div><div class="line">            self.seq_length = seq_length</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.batch_size = <span class="number">1</span></div><div class="line">            self.seq_length = <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'add_input_layer'</span>):</div><div class="line">            self.add_input_layer()</div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'lstm_cell'</span>):</div><div class="line">            self.add_multi_cells()</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'build_output'</span>):</div><div class="line">            self.build_output()</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</div><div class="line">            self.compute_cost()</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'train_op'</span>):</div><div class="line">            self.train_op()</div></pre></td></tr></table></figure><p><strong>题外话</strong> ：tensorflow1.2版本之后，定义多层lstm(<code>MultiRNNCell</code>)与原来的版本改变比较大，可以看考<a href="https://www.tensorflow.org/tutorials/recurrent#recurrent-neural-networks" target="_blank" rel="external">PTB tutorials—Stacking multiple LSTMs</a>.</p><p><em><strong>文中涉及到的代码见：<a href="https://github.com/STHSF/DeepNaturalLanguageProcessing/tree/master/language_model/anna" target="_blank" rel="external">github–anna</a></strong></em></p><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p><a href="http://blog.csdn.net/u014283248/article/details/64440268" target="_blank" rel="external">1、tensorflow1.x版本rnn生成cell 报错解决方案</a></p><p><a href="http://www.cnblogs.com/max-hu/p/7101164.html" target="_blank" rel="external">2、ValueError: Attempt to reuse RNNCell</a></p><p><a href="https://stackoverflow.com/questions/43935609/how-to-reuse-weights-in-multirnncell" target="_blank" rel="external">3、How to reuse weights in MultiRNNCell?</a></p><p><a href="https://github.com/tensorflow/tensorflow/issues/8191" target="_blank" rel="external">4、ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --&gt;&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面&lt;/h1&gt;&lt;p&gt;最近在学习使用t
    
    </summary>
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/categories/Tensorflow/"/>
    
    
      <category term="Tensorflow" scheme="http://sthsf.github.io/tags/Tensorflow/"/>
    
      <category term="Deeplearning" scheme="http://sthsf.github.io/tags/Deeplearning/"/>
    
      <category term="ValueError" scheme="http://sthsf.github.io/tags/ValueError/"/>
    
  </entry>
  
  <entry>
    <title>Errors when buildding blog with hexo</title>
    <link href="http://sthsf.github.io/2017/03/18/Errors%20when%20buildding%20blog%20with%20hexo/"/>
    <id>http://sthsf.github.io/2017/03/18/Errors when buildding blog with hexo/</id>
    <published>2017-03-18T02:51:24.000Z</published>
    <updated>2017-09-04T02:33:52.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --><h2 id="hexo-安装提交过程中错误以及解决方法"><a class="markdownIt-Anchor" href="#hexo-安装提交过程中错误以及解决方法"></a> Hexo 安装提交过程中错误以及解决方法</h2><h4 id="1-错误提示"><a class="markdownIt-Anchor" href="#1-错误提示"></a> 1、错误提示：</h4><figure class="highlight fortran"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ERROR Process failed: _posts/Tensorflow中dynamic-rnn和row-rnn的区别.md</div><div class="line">YAMLException: can not read a block mapping <span class="built_in">entry</span>; a multiline key may not be an <span class="keyword">implicit</span> key at line <span class="number">9</span>, column <span class="number">1</span>:</div></pre></td></tr></table></figure><p>错误原因：原因是我在makrdown文件中的site那块添加categories的时候，后面的冒号使用的是中文状态下的输入。<br>解决方法：中文冒号改写成英文冒号</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="attr">title:</span> <span class="string">Tensorflow中dynamic_rnn和row_rnn的区别</span></div><div class="line"><span class="attr">date:</span> <span class="number">2017</span><span class="bullet">-09</span><span class="bullet">-04</span> <span class="number">08</span><span class="string">:58:38</span></div><div class="line"><span class="attr">catalog:</span> <span class="literal">true</span></div><div class="line"><span class="attr">tags:</span></div><div class="line"><span class="bullet">-</span> <span class="string">Tensorflow</span></div><div class="line"><span class="bullet">-</span> <span class="string">Tensorflow基础知识</span></div><div class="line"><span class="attr">categories:</span></div><div class="line"><span class="bullet">-</span> <span class="string">Tensorflow基础知识</span></div><div class="line"><span class="meta">---</span></div></pre></td></tr></table></figure><p><strong>注意:注意中英文标点符号的问题，还有就是拼写错误，冒号后面的空格等等格式错误，有的时候不注意很容易写错。</strong></p><h4 id="2-错误提示"><a class="markdownIt-Anchor" href="#2-错误提示"></a> 2、错误提示</h4><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">fatal</span>: unable to access <span class="string">'https://github.com/STHSF/sthsf.github.io/'</span>: Could not resolve <span class="attribute">host</span>: github.com</div><div class="line">FATAL Something's wrong. Maybe you can find the solution <span class="attribute">here</span>: <span class="attribute">http</span>:<span class="comment">//hexo.io/docs/troubleshooting.html</span></div></pre></td></tr></table></figure><p>错误原因：可能是网络原因，导致无法连接到github的服务器。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Sep 07 2017 09:58:59 GMT+0800 (CST) --&gt;&lt;h2 id=&quot;hexo-安装提交过程中错误以及解决方法&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#hexo-安装提交过程中错误以
    
    </summary>
    
      <category term="Hexo" scheme="http://sthsf.github.io/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://sthsf.github.io/tags/Hexo/"/>
    
      <category term="Blog" scheme="http://sthsf.github.io/tags/Blog/"/>
    
  </entry>
  
</feed>
