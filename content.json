{"meta":{"title":"personal blog","subtitle":null,"description":null,"author":"Yu Li","url":"http://sthsf.github.io"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2017-08-22T07:28:50.000Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"/404.html","permalink":"http://sthsf.github.io//404.html","excerpt":"","text":""},{"title":"Categories","date":"2017-08-22T07:23:21.000Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"categories/index.html","permalink":"http://sthsf.github.io/categories/index.html","excerpt":"","text":""},{"title":"Books","date":"2017-08-22T14:16:43.000Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"books/index.html","permalink":"http://sthsf.github.io/books/index.html","excerpt":"","text":""},{"title":"About","date":"2017-08-22T15:14:13.000Z","updated":"2017-08-22T15:14:13.000Z","comments":true,"path":"about/index.html","permalink":"http://sthsf.github.io/about/index.html","excerpt":"","text":"李煜I am a self-motivated postgraduate student who is strongly committed to research. A wandering machine learning researcher, bouncing between groups. I want to understand things clearly, and explain them well. If you are interested in my CV, Don’t hesitate to contact me! Research Experience Capacitance tomography technology application and development of the research, 2014.11-2016.05 Marine environment parameters more rapid detection and the key techniques of processing based on GIS, 2013.09-2016.05 长江口附近海岸生态修复及生物资源利用技术及示范, 2013.09-2016.05 海洋信息服务产业科技应用与产业发展研究, 2013.11-2013.12 Integrated Skills 熟悉Python, Scala, Matlab, C++ 理解深度学习,熟悉Tensorflow、Keras等深度学习框架, 使用Tensorflow和Keras解决过实际问题 了解知识图谱相关理论（实体消歧、语义搜索) 了解Spark, Hadoop等大数据挖掘技术,熟悉linux编程环境。 熟悉HBase、redis等主流的no sql数据库 理解分类聚类回归问题, 熟悉SVM, NMF等机器学习算法, 熟悉自然语言理解。 Interested AreaAI, Machine Learning, Deeplearning, NLP… Education&lt;/h2&gt; 硕士, 软件工程, 信息学院, 上海海洋大学, 9/2013-6/2016 学士, 信息与计算科学, 数理学院, 淮阴工学院, 9/2009-6/2013 Honors &amp; Awards&lt;/h2&gt; 国家二等奖, 第十二届“中关村青联杯”全国研究生数学建模竞赛 国家三等奖, 第十一届“华为杯”全国研究生数学建模竞赛 华东赛区三等奖, 第一届全国高校物联网应用创新大赛 三等奖, 淮安市创业计划大赛"},{"title":"Repository","date":"2017-08-22T07:25:25.000Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"repository/index.html","permalink":"http://sthsf.github.io/repository/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-08-22T07:28:39.000Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"tags/index.html","permalink":"http://sthsf.github.io/tags/index.html","excerpt":"","text":""},{"title":"Links","date":"2017-08-22T14:05:11.000Z","updated":"2017-08-15T18:06:22.000Z","comments":true,"path":"links/index.html","permalink":"http://sthsf.github.io/links/index.html","excerpt":"","text":""}],"posts":[{"title":"Tensorflow基础知识---bidirectional_rnn","slug":"Tensorflow基础知识-bidirectional-rnn","date":"2017-08-31T02:17:27.000Z","updated":"2017-09-01T03:03:23.000Z","comments":true,"path":"2017/08/31/Tensorflow基础知识-bidirectional-rnn/","link":"","permalink":"http://sthsf.github.io/2017/08/31/Tensorflow基础知识-bidirectional-rnn/","excerpt":"","text":"写在前面最近在做一些自然语言处理demo的时候遇到了双向RNN，里面的bidirectional_dynamic_rnn还是值得理解下的，故记录下自己的学习心得。 双向RNNs双向RNNs模型是RNN的扩展模型，RNN模型在处理序列模型的学习上主要是依靠上文的信息，双向RNNs模型认为模型的输出不仅仅依靠序列前面的元素，后面的元素对输出也有影响。比如说，想要预测序列中的一个缺失值，我们不仅仅要考虑该缺失值前面的元素，而且要考虑他后面的元素。简单点来将两个RNN堆叠在一起，分别从两个方向计算序列的output和state，而最终的输出则根据两个RNNs的隐藏状态计算，如下图所示。 Figure 1: A bidirectional RNN model 在每一个时间节点$(x_t)$，这个网络有两层神经元，一层从左向右传播，另一层从右向左传播。为了保证任何时刻t都有两层隐层，这个网络需要消耗两倍的存储量来存储权重和偏置等参数。最终的分类结果是由两层RNN隐层组合来产生最终的结果。公式1和2表示双向RNN隐层的数学含义。在这两个关系中唯一不同点是循环的方向不一样。公式3展示了通过总结过去和未来词的表示，使用类别的关系来预测下一个词预测。 双向循环神经网络的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络，而且这两个都连接着一个输出层，这个结构提供给输出层输入序列中每个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层(w1,w3)，隐含层到隐含层自己(w2, w5),向前和向后隐含层到输出层(w4, w6)。值得注意的是：向后和向前隐含层之间没有信息流，这保证了展开图是非循环的。 Figure 2: Bidirectional RNN model unrolling Tensorflow中实现双向RNNs在tensorflow中已经提供了双向RNNs的接口，使用tf.contrib.rnn.bidirectional_dynamic_rnn()这个函数，就可以很方便的实现双向RNN。首先看下接口的一些参数1234567891011121314bidirectional_dynamic_rnn( cell_fw, # 前向 rnn cell cell_bw, # 反向 rnn cell inputs, # 输入序列. sequence_length=None, # 输入序列的实际长度（可选，默认为输入序列的最大长度） initial_state_fw=None, # 前向rnn_cell的初始状态（可选） initial_state_bw=None, # 反向rnn_cell的初始状态（可选） dtype=None, # 初始化和输出的数据类型（可选） parallel_iterations=None, swap_memory=False, time_major=False, # 决定了输入输出tensor的格式：如果为true, 向量的形状必须为 `[max_time, batch_size, depth]`. # 如果为false, tensor的形状必须为`[batch_size, max_time, depth]`. 与dynamic_rnn中的time_major类似。 scope=None) 函数的返回值：一个（outputs, outputs_state）的一个元祖。其中， outputs=(outputs_fw, outputs_bw),是一个包含前向cell输出tensor和后向tensor输出tensor组成的元祖。若time_major=false，则两个tensor的shape为[batch_size, max_time, depth]，应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。最终的outputs需要使用tf.concat(outputs, 2)将两者合并起来。 outputs_state = (outputs_state_fw， output_state_bw),包含了前向和后向最后的隐藏状态的组成的元祖。outputs_state_fw和output_state_bw的类型都是LSTMStateTuple。LSTMStateTuple由(c, h)组成，分别代表memory cell和hidden state cell_fw和cell_bw的定义是完全一样的，如果两个cell都定义成LSTM就变成说了双向LSTM了。12345import tensorflow as tf # 正向传播的rnn_cellcell_fw_lstm = tf.nn.rnn_cell.LSTMCell('num_units')# 反向传播的rnn_cellcell_bw_lstm = tf.nn.rnn_cell.LSTMCell('num_units') 在bidirectional_dynamic_rnn函数内部，会通过array_ops.reverse_sequence函数将输入序列逆序排列，使其达到反向传播的效果。在实现的时候，我们只需要将定义好的两个cell作为参数传入就可以了：12(outputs, outputs_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw_lstm, cell_bw_lstm, embedding_chars)embedded_chars为输入的tensor，[batch_szie, max_time, depth]。batch_size为模型当中batch的大小，应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。 最终的输出outputs = tf.concat((outputs_fw, outputs_bw), 2)或者直接是outputs = tf.concat(outputs, 2) 如果还需要用到最后的输出状态，则需要对（outputs_state_fw， output_state_bw）处理:1234final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), 1)final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), 1)outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c, h=final_state_h) 双向LSRTM的实现过如下：123456789101112131415161718192021import tensorflow as tf vocab_size = 1000embedding_size = 50batch_size =100max_time = 10hidden_units = 10inputs = tf.placeholder(shape=(batch_size, max_time), dtype=tf.int32, name='inputs')embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), dtype=tf.float32)inputs_embeded = tf.nn.embedding_lookup(embedding, inputs)lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)((outputs_fw, outputs_bw), (outputs_state_fw, outputs_state_bw)) = tf.nn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, inputs_embeded, sequence_length=max_time)outputs = tf.concat((outputs_fw, outputs_bw), 2)final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), 1)final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), 1)outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c, h=final_state_h)","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/categories/Tensorflow/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"基础知识","slug":"基础知识","permalink":"http://sthsf.github.io/tags/基础知识/"}]},{"title":"极大似然估计学习总结","slug":"极大似然估计学习总结","date":"2017-08-24T02:09:46.000Z","updated":"2017-08-24T02:32:27.000Z","comments":true,"path":"2017/08/24/极大似然估计学习总结/","link":"","permalink":"http://sthsf.github.io/2017/08/24/极大似然估计学习总结/","excerpt":"","text":"写在前面极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这也样本分布的参数，把可能性最大的那个参数作为真实参数估计。 极大似然估计的原理给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。 参考文献似然函数如何理解似然函数?对似然函数的理解最大似然估计总结笔记最大似然估计","categories":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/categories/统计学习/"}],"tags":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/tags/统计学习/"},{"name":"概率论","slug":"概率论","permalink":"http://sthsf.github.io/tags/概率论/"}]},{"title":"ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists.","slug":"ValueError: kernel already exists","date":"2017-06-18T02:00:00.000Z","updated":"2017-08-31T05:40:31.000Z","comments":true,"path":"2017/06/18/ValueError: kernel already exists/","link":"","permalink":"http://sthsf.github.io/2017/06/18/ValueError: kernel already exists/","excerpt":"","text":"写在前面最近在学习使用tensorflow构建language model，遇到关于模型重用的问题，我将模型的训练和预测放在同一个文件中时出现的问题,提示lstm_cell kernal已经存在. 错误提示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606148 Traceback (most recent call last): 49 File \"anna_writer.py\", line 274, in &lt;module&gt; 50 samp = generate_samples(checkpoint, 20000, prime=\"The \") 51 File \"anna_writer.py\", line 234, in generate_samples 52 conf.lstm_size, conf.keep_prob, conf.grad_clip, False) 53 File \"anna_writer.py\", line 74, in __init__ 54 self.add_lstm_cell() 55 File \"anna_writer.py\", line 110, in add_lstm_cell 56 initial_state=self.initial_state) 57 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 574, ii n dynamic_rnn 58 dtype=dtype) 59 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 737, ii n _dynamic_rnn_loop 60 swap_memory=swap_memory) 61 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2770, in while_loop 62 result = context.BuildLoop(cond, body, loop_vars, shape_invariants) 63 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2599, in BuildLoop 64 pred, body, original_loop_vars, loop_vars, shape_invariants) 65 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2549, in _BuildLoop 66 body_result = body(*packed_vars_for_body) 67 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 722, ii n _time_step 68 (output, new_state) = call_cell() 69 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 708, ii n &lt;lambda&gt; 70 call_cell = lambda: cell(input_t, state) 71 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", ll ine 180, in __call__ 72 return super(RNNCell, self).__call__(inputs, state) 73 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 444 1, in __call__ 74 outputs = self.call(inputs, *args, **kwargs) 75 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 916, in call 76 cur_inp, new_state = cell(cur_inp, cur_state) 77 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 752, in __call__ 78 output, new_state = self._cell(inputs, state, scope) 79 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 180, in __call__ 80 return super(RNNCell, self).__call__(inputs, state) 81 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 44 1, in __call__ 82 outputs = self.call(inputs, *args, **kwargs) 83 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 383, in call 84 concat = _linear([inputs, h], 4 * self._num_units, True) 85 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 1017, in _linear 86 initializer=kernel_initializer) 87 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable 88 use_resource=use_resource, custom_getter=custom_getter) 89 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable 90 use_resource=use_resource, custom_getter=custom_getter) 91 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable 92 validate_shape=validate_shape, use_resource=use_resource) 93 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1405, in wrapped_custom_getter 94 *args, **kwargs) 95 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 183, in _rnn_get_variable 96 variable = getter(*args, **kwargs) 97 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 183, in _rnn_get_variable 98 variable = getter(*args, **kwargs) 99 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter 100 use_resource=use_resource) 101 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 664, in _get_single_variable 102 name, \"\".join(traceback.format_list(tb)))) 103 ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:104 105 File \"anna_writer.py\", line 110, in add_lstm_cell 106 initial_state=self.initial_state) 107 File \"anna_writer.py\", line 74, in __init__ 108 self.add_lstm_cell() 109 File \"anna_writer.py\", line 172, in train 110 conf.grad_clip, is_training=True) 解决方法这个问题困扰了我两天，始终找不到解决方案，当我的训练模型和预测模型分开运行时程序没有报错，但是两个程序放在一起运行时就会出现问题，网上搜索的结果大都是关于共享权重的问题，错误提示是: 1ValueError: Variable hello/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? 这个error跟我的错误还是有一定的区别的。这些问题主要原因在于使用多层lstm_cell或者双向lstm的时候忽略了定义变量的variable_scope，导致lstm_cell的作用域不一样，但是程序加载的时候并不知道，所以当声明的cell不是同一个的时候，需要用 1with tf.variable_scope(name): 来定义不同的作用范围就可以了，具体还要根据实际情况。 而我的问题好像网上还没有这样的解释，我仔细看错误的提示，分析我的代码，当train和predict放在一起的时候，会调用两次class language_model：这时候就会出现系统里应该存在两个不同的lstm_cell模型，但是系统无法辨别出来，所以会提示kernel already exists，而不是weights already exists。 而tensorflow有一个reset_default_graph()函数，我对python多线程不是很清楚，贴下源码， 123456789def reset_default_graph(): \"\"\"Clears the default graph stack and resets the global default graph. NOTE: The default graph is a property of the current thread. This function applies only to the current thread. Calling this function while a `tf.Session` or `tf.InteractiveSession` is active will result in undefined behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects after calling this function will result in undefined behavior. \"\"\" 然后在我定义的language_model类中添加这个函数之后之前的问题就解决了。 12345678910111213141516171819202122232425262728293031import tensorflow as tfclass language_model: def __init__(self, num_classes, batch_size=100, seq_length=50, learning_rate=0.01, num_layers=5, hidden_units=128, keep_prob=0.8, grad_clip=5, is_training=True): # 模型的训练和预测放在同一个文件下时如果没有这个函数会报错。 tf.reset_default_graph() self.learning_rate = learning_rate self.num_layers = num_layers self.hidden_units = hidden_units self.is_training = is_training self.keep_prob = keep_prob self.grad_clip = grad_clip self.num_classes = num_classes if self.is_training: self.batch_size = batch_size self.seq_length = seq_length else: self.batch_size = 1 self.seq_length = 1 with tf.name_scope('add_input_layer'): self.add_input_layer() with tf.variable_scope('lstm_cell'): self.add_multi_cells() with tf.name_scope('build_output'): self.build_output() with tf.name_scope('cost'): self.compute_cost() with tf.name_scope('train_op'): self.train_op() 题外话 ：tensorflow1.2版本之后，定义多层lstm(MultiRNNCell)与原来的版本改变比较大，可以看考PTB tutorials—-Stacking multiple LSTMs. 文中涉及到的代码见：github—anna 参考文献1、tensorflow1.x版本rnn生成cell 报错解决方案 2、ValueError: Attempt to reuse RNNCell 3、How to reuse weights in MultiRNNCell? 4、ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/categories/Tensorflow/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Deeplearning","slug":"Deeplearning","permalink":"http://sthsf.github.io/tags/Deeplearning/"},{"name":"ValueError","slug":"ValueError","permalink":"http://sthsf.github.io/tags/ValueError/"}]},{"title":"Build personal blog with hexo","slug":"Build personal blog with hexo","date":"2017-03-18T02:51:24.000Z","updated":"2017-08-22T13:38:51.000Z","comments":true,"path":"2017/03/18/Build personal blog with hexo/","link":"","permalink":"http://sthsf.github.io/2017/03/18/Build personal blog with hexo/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://sthsf.github.io/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://sthsf.github.io/tags/Hexo/"},{"name":"Blog","slug":"Blog","permalink":"http://sthsf.github.io/tags/Blog/"}]}]}