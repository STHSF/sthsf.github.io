{"meta":{"title":"Yu Li's personal blog","subtitle":null,"description":null,"author":"Yu Li","url":"http://sthsf.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2017-10-20T00:52:34.329Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"/404.html","permalink":"http://sthsf.github.io//404.html","excerpt":"","text":""},{"title":"About","date":"2019-12-15T23:59:13.234Z","updated":"2019-12-15T23:59:13.234Z","comments":true,"path":"about/index.html","permalink":"http://sthsf.github.io/about/index.html","excerpt":"","text":"李煜I am a self-motivated postgraduate student who is strongly committed to research.A wandering machine learning researcher, bouncing between groups.I want to understand things clearly, and explain them well.If you are interested in my CV, and my English CV, Don’t hesitate to contact me! Interested AreaMachine Learning, Deeplearning, NLP... Integrated Skills熟练掌握Python、Scala、Matlab、C++开发。理解深度学习,熟悉Tensorflow、pytorch等深度学习框架;理解分类聚类回归问题, 熟悉SVM, NMF等常用的机器学习算法, 熟悉自然语言理解;了解知识图谱相关理论（实体消歧、语义搜索等）;了解Spark, Hadoop等大数据计算框架, 了解日志监控系统ElasticSearch;熟悉Linux原理和Docker技术,了解Docker相关技术生态;熟悉HBase、redis等主流的no sql数据库. Open Source1、Keywords/Phase/Summary Extract Based On TextRank基于TextRank的关键词、短语、摘要提取程序，代码使用Scala编写。2、Text Classification based on word2vecText Classification Based on Word2Vec and TextRank3、Chinese Word Segmentation基于bidirectional_lstm的中文分词系统4、Text Generation基于lstm的languag model5、Sentiment Analysis文本情感分类6、stockindex500Trend Prediction for High Frequency Trading基于过去30个1分钟量价数据预测未来20分钟后的涨跌方向7、金融财经类新闻文本主题事件提取 Honors &amp; Awards国家二等奖, 第十二届“中关村青联杯”全国研究生数学建模竞赛国家三等奖, 第十一届“华为杯”全国研究生数学建模竞赛华东赛区三等奖, 第一届全国高校物联网应用创新大赛三等奖, 淮安市创业计划大赛 Research ExperienceCapacitance tomography technology application and development of the research, 2014.11-2016.05Marine environment parameters more rapid detection and the key techniques of processing based on GIS, 2013.09-2016.05长江口附近海岸生态修复及生物资源利用技术及示范, 2013.09-2016.05海洋信息服务产业科技应用与产业发展研究, 2013.11-2013.12 PUBLICATION“Unmixing: A New Direction From Classical Tidal Harmonic Analysis” [J], 2015 International Academic Conference, MTS / IEEE OCEANS’15, Washington, DC, first author, thesis retrieved by EI, 2015 (9).The non-negative matrix factorization (NMF) algorithm is applied to the unsupervised blind separation of tidal tide, which solves the problems of difficult data collection and complicated calculation encountered in traditional separation.&quot; blind signal separation algorithm based on NMF with Projection Gradient&quot; [J], first author, computer engineering, 2016 (02).“Relationship between mouse brain wave and respiration based on wavelet support vector machine” [J] .Chinese Journal of Medical Physics. 2015 (03).“Weight Analysis of Influencing Factors of Green Tide Disasters Based on Support Vector Machines” [J]. China Environmental Science. 2015 (11)."},{"title":"Books","date":"2017-10-20T00:52:34.351Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"books/index.html","permalink":"http://sthsf.github.io/books/index.html","excerpt":"","text":""},{"title":"Categories","date":"2017-10-20T00:52:34.352Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"categories/index.html","permalink":"http://sthsf.github.io/categories/index.html","excerpt":"","text":""},{"title":"Links","date":"2017-10-20T00:52:34.353Z","updated":"2017-08-15T18:06:22.000Z","comments":true,"path":"links/index.html","permalink":"http://sthsf.github.io/links/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-10-20T00:52:34.355Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"tags/index.html","permalink":"http://sthsf.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repository","date":"2017-10-20T00:52:34.354Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"repository/index.html","permalink":"http://sthsf.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"命名实体识别(Name Entity Recognition)综述","slug":"NLP--命名实体识别","date":"2020-02-18T07:37:46.000Z","updated":"2020-02-18T10:23:06.231Z","comments":true,"path":"2020/02/18/NLP--命名实体识别/","link":"","permalink":"http://sthsf.github.io/2020/02/18/NLP--命名实体识别/","excerpt":"","text":"命名实体识别识别对话里面的人名、地名、组织机构名。属于序列标注问题。 命名实体识别中的标签集合命名实体识别的标注采用的是BIEO的方式，即Begin，Intermediate，End，Other。实际上也就是使用BIEO将目标句中的词按照需求的方式标记，不同的结果取决于对样本数据的标注，一般序列的标注是要符合一定的标注标准的如(PKU数据标注规范)。词性标注、分词都属于同一类问题，他们的区别主要是标注的方式不同。标注方式1:LabelSet = {BA, MA, EA, BO, MO, EO, BP, MP, EP, O}其中，BA代表这个汉字是地址首字，MA代表这个汉字是地址中间字，EA代表这个汉字是地址的尾字；BO代表这个汉字是机构名的首字，MO代表这个汉字是机构名称的中间字，EO代表这个汉字是机构名的尾字；BP代表这个汉字是人名首字，MP代表这个汉字是人名中间字，EP代表这个汉字是人名尾字，而O代表这个汉字不属于命名实体。标注方式2:LabelSet = {NA, SC, CC, SL, LL, SP, PP}其中 NA = No entity, SC = Start Company, CC = Continue Company, SL = Start Location, CL = Continue Location, SP = Start Person, CP = Continue Person标注方式3:LabelSet = {O, B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG}其中，PER代表人名， LOC代表位置， ORG代表组织. B-PER、I-PER代表人名首字、人名非首字，B-LOC、I-LOC代表地名首字、地名非首字，B-ORG、I-ORG代表组织机构名首字、组织机构名非首字，O代表该字不属于命名实体的一部分。这是一种基于字级别的标注方式一般来说，NER的标注列表为[‘O’ ,‘B-MISC’, ‘I-MISC’, ‘B-ORG’ ,‘I-ORG’, ‘B-PER’ ,‘I-PER’, ‘B-LOC’ ,‘I-LOC’]。其中，一般一共分为四大类：PER（人名），LOC（位置[地名]），ORG（组织）以及MISC(杂项)，而且B表示开始，I表示中间，O表示单字词。 參考文獻NER基础却不简单，命名实体识别的难点与现状python+HMM之维特比解码Tagging Problems, and Hidden Markov Models 基于字符的BiLSTM-CRF模型 常见错误 参考文献","categories":[{"name":"NLP","slug":"NLP","permalink":"http://sthsf.github.io/categories/NLP/"}],"tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://sthsf.github.io/tags/自然语言处理/"},{"name":"NLP","slug":"NLP","permalink":"http://sthsf.github.io/tags/NLP/"}]},{"title":"工具类介绍--github","slug":"工具类介绍--github","date":"2020-02-14T02:09:46.000Z","updated":"2020-02-15T09:52:57.723Z","comments":true,"path":"2020/02/14/工具类介绍--github/","link":"","permalink":"http://sthsf.github.io/2020/02/14/工具类介绍--github/","excerpt":"","text":"写在前面 常见错误 错误11234567To https://github.com/STHSF/DeepNaturalLanguageProcessing.git ! [rejected] develop -&gt; develop (non-fast-forward)error: failed to push some refs to 'https://github.com/STHSF/DeepNaturalLanguageProcessing.git'hint: Updates were rejected because a pushed branch tip is behind its remotehint: counterpart. Check out this branch and integrate the remote changeshint: (e.g. 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. 参考文献","categories":[{"name":"github","slug":"github","permalink":"http://sthsf.github.io/categories/github/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://sthsf.github.io/tags/工具/"},{"name":"服务端","slug":"服务端","permalink":"http://sthsf.github.io/tags/服务端/"}]},{"title":"python与协程","slug":"python基础知识--协程","date":"2019-08-24T02:09:46.000Z","updated":"2020-02-15T09:52:56.017Z","comments":true,"path":"2019/08/24/python基础知识--协程/","link":"","permalink":"http://sthsf.github.io/2019/08/24/python基础知识--协程/","excerpt":"","text":"写在前面在执行一些IO密集型任务的时候, 程序往往会因为等待IO而阻塞. 比如在网络爬虫中, 如果我们使用requests函数库来进行请求的话, 如果网站的响应速度过慢, 程序会一直在等待网站的响应, 最后导致爬虫爬取的效率非常低.协程可以用来进行加速, 它对于IO密集型仍无非常有效. 基本概念在了解协程之前, 我们得了解阻塞, 非阻塞, 同步, 异步, 多进程, 协程等基本概念 1、阻塞阻塞状态是指程序未得到所需计算资源时被挂起的状态. 程序在等待某个操作完成期间, 自身无法继续干别的事情, 则称该程序在该操作上是阻塞的.常见的阻塞形式有: 网络I/O阻塞, 磁盘I/O阻塞, 用户输入阻塞等, 阻塞无处不在, 包括CPU切换上下文时, 所有的进程都无法真正干事情, 他们也会被阻塞, 如果是多核CPU, 则正在执行上下文切换操作的核不可被利用. 2、非阻塞程序在等待某些操作过程中, 自身不被阻塞, 可以继续干别的事情, 则称该程序在该操作上是非阻塞的.非阻塞并不是在任何程序级别、任何情况下都可以存在, 仅当程序封装的级别可以囊括独立子程序单元时, 它才可能存在非阻塞状态.非阻塞的存在是因为阻塞的存在, 正是因为某个操作阻塞导致的耗时与效率地下, 我们才要把它变成非阻塞的. 3、 同步不同程序单元为了完成某个任务, 在执行过程中需要靠某种通信方式以协调一致, 称这些程序单元是同步执行的.简言之, 同步意味着有序. 4、异步为了完成某个任务, 不同程序单元之间过程中无需通信协调, 也能完成任务的方式, 不相关的程序单元之间可以是异步的.简言之, 异步意味着无序. 5、多进程多进程就是利用CPU的多核优势, 在同一时间并行的执行多个任务, 可以大大提高执行效率. 6、协程协程是一种用户态的轻量级线程.协程拥有自己的寄存器上下文和栈. 协程调度切换时, 将寄存器上下文和栈保存到其他地方, 在切回来的时候, 恢复先前保存的寄存器上下文和栈, 直接操作栈则基本没有内核切换的开销, 可以不加锁的访问全局变量, 所以上下文切换非常快. 因此协程能保存上次调用时的状态, 即所有局部状态的一个特定组合, 每次过程重入时, 就相当于进入上一次调用的状态.协程本质上是个单进程, 协程相对于多进程来说, 无须线程上下文切换的开销, 无需原子操作锁定及同步的开销, 编程模型也非常简单.我们可以通过协程来实现异步操作, 不如网络爬虫的场景下, 我们发出一个请求之后, 需要等待一定的时间才能得到响应, 但其实在这个等待的过程中, 程序可以干许多其他的事情, 等到响应得到之后才切换回来继续处理, 这样可以充分利用CPU和其他资源, 这就是异步协程的优势.协程的作用是在执行函数A时, 可以随时中断, 去执行另一个函数B, 然后中断继续执行函数A(自由切换). 但这一过程中并不是函数调用(没有调用语句). 关于同步异步阻塞非阻塞的通俗理解老张爱喝茶，废话不说，煮开水。出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。1 老张把水壶放到火上，立等水开。（同步阻塞）老张觉得自己有点傻2 老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞）老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。3 老张把响水壶放到火上，立等水开。（异步阻塞）老张觉得这样傻等意义不大4 老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞）老张觉得自己聪明了。所谓同步异步，只是对于水壶而言。普通水壶，同步；响水壶，异步。虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。所谓阻塞非阻塞，仅仅对于老张而言。立等的老张，阻塞；看电视的老张，非阻塞。情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import geventimport time# 并发下载示例def task(pid, fac): response = urllib2.urlopen('http://json-time.appspot.com/time.json') result = response.read() json_result = json.loads(result) datetime = json_result['datetime'] print('Process %s %s' % (pid, datetime)) return json_result['datetime']lists = range(0,10000)fac = 'task name'def synchronous(): # 同步一个线程执行函数 t = time.time() for i in lists: task(i, fac) print(time.time() - t)def asynchronous(): # 异步一个线程执行函数 t = time.time() threads = [gevent.spawn(task, i, fac) for i in lists] # gevent.spawn 时，协程已经开始执行 gevent.joinall(threads) # gevent.joinall 只是用来等待所有协程执行完毕 print(time.time() - t)# def asynchornoius2():# t = time.time()# x = Pool(40)# for i in lists:# x.imap(task)# # x.join()# print(time.time() - t)### def asynchornoius3():# t = time.time()# x = Pool(40)# for i in lists:# x.imap_unordered(task, i, fac)# x.join()# print(time.time() - t)print('synchronous:')synchronous() # 同步执行时要等待执行完后再执行print('asynchronous:')asynchronous() # 异步时遇到等待则会切换执行print('asynchronius:')asynchornoius()# print('-----------')# asynchornoius2()# print('-----------')## asynchornoius3()123456789101112131415161718import geventfrom gevent.pool import Poolfrom gevent import monkey; monkey.patch_all()# 限制并发的 greenletspool = Pool(300)def asynchornoius(): t = time.time() greenlets = [] for i in lists: greenlet = pool.spawn(task, i, fac) greenlets.append(greenlet) pool.join() print(time.time() - t) # 协程中避免使用全局变量来进行状态统计，或者结果收集。 # 若要收集结果，可以使用 value 属性来获取每个 greenlet 的返回值。 for greenlet in greenlets: print greenlet.value 参考文献Python中异步协程的使用方法介绍gevent程序员指南gevent Tricks关于gevent的Timeout(超时)问题……Gevent 性能和 gevent.loop 的运用和带来的思考延时和锁python中的协程深入理解对Python协程之异步同步的区别详解python3之协程patch_all 不是一个好主意","categories":[{"name":"python基础知识","slug":"python基础知识","permalink":"http://sthsf.github.io/categories/python基础知识/"}],"tags":[{"name":"python","slug":"python","permalink":"http://sthsf.github.io/tags/python/"}]},{"title":"智能问答系统的探索和实践","slug":"智能问答系统的探索和实践","date":"2019-03-16T02:09:46.000Z","updated":"2019-09-19T01:10:48.296Z","comments":true,"path":"2019/03/16/智能问答系统的探索和实践/","link":"","permalink":"http://sthsf.github.io/2019/03/16/智能问答系统的探索和实践/","excerpt":"","text":"写在前面知识图谱 (Knowledge Graph) 是当前的研究热点。自从2012年Google推出自己第一版知识图谱以来，它在学术界和工业界掀起了一股热潮。各大互联网企业在之后的短短一年内纷纷推出了自己的知识图谱产品以作为回应。那么与这些传统的互联网公司相比，对处于当今风口浪尖上的行业 - 互联网金融， 知识图谱可以有哪方面的应用呢？ 用户画像 参考文献这是一份通俗易懂的知识图谱技术与应用指南知识图谱在互联网金融行业的应用知识图谱的技术与应用","categories":[{"name":"智能问答系统","slug":"智能问答系统","permalink":"http://sthsf.github.io/categories/智能问答系统/"}],"tags":[{"name":"智能问答系统","slug":"智能问答系统","permalink":"http://sthsf.github.io/tags/智能问答系统/"},{"name":"综述","slug":"综述","permalink":"http://sthsf.github.io/tags/综述/"}]},{"title":"知识图谱的基础知识和应用场景","slug":"知识图谱的基础知识和应用场景","date":"2019-03-15T02:09:46.000Z","updated":"2019-03-18T05:56:49.000Z","comments":true,"path":"2019/03/15/知识图谱的基础知识和应用场景/","link":"","permalink":"http://sthsf.github.io/2019/03/15/知识图谱的基础知识和应用场景/","excerpt":"","text":"写在前面知识图谱 (Knowledge Graph) 是当前的研究热点。自从2012年Google推出自己第一版知识图谱以来，它在学术界和工业界掀起了一股热潮。各大互联网企业在之后的短短一年内纷纷推出了自己的知识图谱产品以作为回应。那么与这些传统的互联网公司相比，对处于当今风口浪尖上的行业 - 互联网金融， 知识图谱可以有哪方面的应用呢？ 聊天机器人聊天机器人对知识图谱有哪些特殊的需求？给定一个概率分布(D)(D)(D)，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为(fD)(f_D)(f​D​​),以及一个分布参数，我们可以从这个分布中抽出一个具有(n)(n)(n)个值的采样。 参考文献这是一份通俗易懂的知识图谱技术与应用指南知识图谱在互联网金融行业的应用知识图谱的技术与应用kg-beijing知识图谱基础","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://sthsf.github.io/categories/知识图谱/"}],"tags":[{"name":"综述","slug":"综述","permalink":"http://sthsf.github.io/tags/综述/"},{"name":"知识图谱","slug":"知识图谱","permalink":"http://sthsf.github.io/tags/知识图谱/"}]},{"title":"Tensorflow中的共享变量机制","slug":"Tensorflow中的共享变量机制","date":"2017-09-14T02:14:56.000Z","updated":"2019-09-17T01:30:35.899Z","comments":true,"path":"2017/09/14/Tensorflow中的共享变量机制/","link":"","permalink":"http://sthsf.github.io/2017/09/14/Tensorflow中的共享变量机制/","excerpt":"","text":"写在前面本文总结了Tensorflow使用过程中易混淆的一些接口，比如get_variable()和Variable()。name_scope()和variable_scope()等。 tf.get_varibale()/ tf.Variable()tensorflow中关于variable的op有tf.get_variable()和tf.Variable两个. tf.get_variable()123456789101112tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None) tf.Variable()12345678910tf.Variable(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None) tf.get_variable()/tf.Variable()的区别先看下面的两个例子：12345import tensorflow as tf a1 = tf.Variable(0, name=\"a1\")a2 = tf.Variable(1, name='a1')print(a1.name)print(a2.name)输出结果12a1:0a1_1:012345import tensorflow as tfa1 = tf.get_variable(\"a1\", 0)a2 = tf.get_variable('a1', 0)print(a1.name)print(a2.name)输出结果：1ValueError: Variable a1 already exists, disallowed. Did you mean to set reuse=True in VarScope?使用tf.Varibale()定义变量的时候，如果检测到命名冲突，系统会自动解决，但是使用tf.get_varibale()时，系统不会解决冲突，并且会报错。所以如果需要共享变量则需要使用tf.get_variable()。在其他情况下两者的用法基本一样。我们再来看一段代码:12345678910import tensorflow as tfwith tf.variable_scope(\"scope1\"): w1 = tf.get_variable(\"w1\", shape=[]) w2 = tf.Variable(0.0, name=\"w2\")with tf.variable_scope(\"scope1\", reuse=True): w1_p = tf.get_variable(\"w1\", shape=[]) w2_p = tf.Variable(1.0, name=\"w2\") print(w1 is w1_p, w2 is w2_p)输出结果：1(True, False)从输出结果可以看出，对于get_variable()，来说，如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的。而tf.Variable()每次都在创建新对象。这里没有太多的提到共享变量的问题， tf.name_scope()/ tf.variable_scope() 参考文献tensorflow学习笔记（二十三）：variable与get_variable共享变量","categories":[{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/categories/Tensorflow基础知识/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/tags/Tensorflow基础知识/"}]},{"title":"Tensorflow中dynamic_rnn和row_rnn的区别","slug":"Tensorflow中dynamic-rnn和row-rnn的区别","date":"2017-09-04T00:58:38.000Z","updated":"2017-09-14T01:28:17.000Z","comments":true,"path":"2017/09/04/Tensorflow中dynamic-rnn和row-rnn的区别/","link":"","permalink":"http://sthsf.github.io/2017/09/04/Tensorflow中dynamic-rnn和row-rnn的区别/","excerpt":"","text":"写在前面 参考文献tensor flow dynamic_rnn 与rnn有啥区别？有大神能详细讲讲tensorflow中的raw_rnn这个函数么？","categories":[{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/categories/Tensorflow基础知识/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/tags/Tensorflow基础知识/"}]},{"title":"Tensorflow基础知识---Bidirectional_RNN","slug":"Tensorflow基础知识-bidirectional-rnn","date":"2017-08-31T02:17:27.000Z","updated":"2017-09-14T02:22:59.000Z","comments":true,"path":"2017/08/31/Tensorflow基础知识-bidirectional-rnn/","link":"","permalink":"http://sthsf.github.io/2017/08/31/Tensorflow基础知识-bidirectional-rnn/","excerpt":"","text":"写在前面最近在做一些自然语言处理demo的时候遇到了双向RNN，里面的bidirectional_dynamic_rnn和static_bidirectional_rnn还是值得理解下的，故记录下自己的学习心得。 双向RNNs双向RNNs模型是RNN的扩展模型，RNN模型在处理序列模型的学习上主要是依靠上文的信息，双向RNNs模型认为模型的输出不仅仅依靠序列前面的元素，后面的元素对输出也有影响。比如说，想要预测序列中的一个缺失值，我们不仅仅要考虑该缺失值前面的元素，而且要考虑他后面的元素。简单点来将两个RNN堆叠在一起，分别从两个方向计算序列的output和state，而最终的输出则根据两个RNNs的隐藏状态计算，如下图所示。Figure 1: A bidirectional RNN model在每一个时间节点(xt)(x_t)(x​t​​)，这个网络有两层神经元，一层从左向右传播，另一层从右向左传播。为了保证任何时刻t都有两层隐层，这个网络需要消耗两倍的存储量来存储权重和偏置等参数。最终的分类结果是由两层RNN隐层组合来产生最终的结果。公式1和2表示双向RNN隐层的数学含义。在这两个关系中唯一不同点是循环的方向不一样。公式3展示了通过总结过去和未来词的表示，使用类别的关系来预测下一个词预测。双向循环神经网络的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络，而且这两个都连接着一个输出层，这个结构提供给输出层输入序列中每个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层(w1,w3)，隐含层到隐含层自己(w2, w5),向前和向后隐含层到输出层(w4, w6)。值得注意的是：向后和向前隐含层之间没有信息流，是独立计算的，只是最后输出的时候把二者的状态向量结合起来，这保证了展开图是非循环的。Figure 2: Bidirectional RNN model unrolling Tensorflow中实现双向RNNs tf.contrib.rnn.bidirectional_dynamic_rnn()在tensorflow中已经提供了双向RNNs的接口，使用tf.contrib.rnn.bidirectional_dynamic_rnn()这个函数，就可以很方便的构建双向RNN网络。首先看下接口的一些参数1234567891011121314bidirectional_dynamic_rnn( cell_fw, # 前向 rnn cell cell_bw, # 反向 rnn cell inputs, # 输入序列. sequence_length=None, # 输入序列的实际长度（可选，默认为输入序列的最大长度） initial_state_fw=None, # 前向rnn_cell的初始状态（可选） initial_state_bw=None, # 反向rnn_cell的初始状态（可选） dtype=None, # 初始化和输出的数据类型（可选） parallel_iterations=None, swap_memory=False, time_major=False, # 决定了输入输出tensor的格式：如果为true, 向量的形状必须为 `[max_time, batch_size, depth]`. # 如果为false, tensor的形状必须为`[batch_size, max_time, depth]`. 与dynamic_rnn中的time_major类似。 scope=None)函数的返回值：一个（outputs, outputs_state）的一个元祖。其中，outputs=(outputs_fw, outputs_bw),是一个包含前向cell输出tensor和后向tensor输出tensor组成的元祖。若time_major=false，则两个tensor的shape为[batch_size, max_time, depth]，应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。最终的outputs需要使用tf.concat(outputs, 2)将两者合并起来。outputs_state = (outputs_state_fw， output_state_bw),包含了前向和后向最后的隐藏状态的组成的元祖。outputs_state_fw和output_state_bw的类型都是LSTMStateTuple。LSTMStateTuple由(c, h)组成，分别代表memory cell和hidden statecell_fw和cell_bw的定义是完全一样的，如果两个cell都定义成LSTM就变成说了双向LSTM了。12345import tensorflow as tf # 正向传播的rnn_cell单元，这里使用的是LSRTMCellcell_fw_lstm = tf.nn.rnn_cell.LSTMCell('num_units')# 反向传播的rnn_cell单元，与正向传播的rnn单元相同cell_bw_lstm = tf.nn.rnn_cell.LSTMCell('num_units')在bidirectional_dynamic_rnn函数内部，会通过array_ops.reverse_sequence函数将输入序列逆序排列，使其达到反向传播的效果。在实现的时候，我们只需要将定义好的两个cell作为参数传入就可以了：123(outputs, outputs_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw_lstm, cell_bw_lstm, inputs_embedded)# inputs_embedded为输入的tensor，[batch_szie, max_time, depth]。batch_size为模型当中batch的大小.# 应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。最终的输出outputs = tf.concat((outputs_fw, outputs_bw), 2)或者直接是outputs = tf.concat(outputs, 2)如果还需要用到最后的输出状态，则需要对（outputs_state_fw， output_state_bw）处理:1234final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), 1)final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), 1)outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c, h=final_state_h) 双向LSRTM的实现过如下：123456789101112131415161718192021import tensorflow as tf vocab_size = 1000embedding_size = 50batch_size =100max_time = 10hidden_units = 10inputs = tf.placeholder(shape=(batch_size, max_time), dtype=tf.int32, name='inputs')embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), dtype=tf.float32)inputs_embeded = tf.nn.embedding_lookup(embedding, inputs)lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)((outputs_fw, outputs_bw), (outputs_state_fw, outputs_state_bw)) = tf.nn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, inputs_embeded, sequence_length=max_time)outputs = tf.concat((outputs_fw, outputs_bw), 2)final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), 1)final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), 1)outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c, h=final_state_h) tf.contrib.rnn.static_bidirectional_rnn() 多层双向RNNs图三展示了一个从较低层传播到下层的多层双向RNN。如图所示，在网络结构中，第t个时间里每一个中间神经元接受到前一个时间（同样的RNN层）传递过来的一组参数，以及之前RNN层传递过来的两组参数。这两组参数一个是从左到右的RNN输入，另一个是从右到左的RNN输入。Figure 3: Multi-Bidirectional RNN model为了构建一个L层的RNN，上述的关系将会参照公式4和公式5所修改，其中每一个中间神经元（第i层）的输入是RNN网络中同样的t时刻第i-1层的输出。其中输出，在每一个时刻值为通过所有隐层的输入参数传播的结果（如公式6所示）。 参考文献tensorflow.nn.bidirectional_dynamic_rnn()函数的用法深度学习与自然语言处理(7)_斯坦福cs224d 语言模型，RNN，LSTM与GRUtensorflow学习笔记(三十九):双向rnn","categories":[{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/categories/Tensorflow基础知识/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/tags/Tensorflow基础知识/"}]},{"title":"极大似然估计学习总结","slug":"极大似然估计学习总结","date":"2017-08-24T02:09:46.000Z","updated":"2019-03-14T06:51:45.027Z","comments":true,"path":"2017/08/24/极大似然估计学习总结/","link":"","permalink":"http://sthsf.github.io/2017/08/24/极大似然估计学习总结/","excerpt":"","text":"写在前面极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这也样本分布的参数，把可能性最大的那个参数作为真实参数估计。 极大似然估计的原理给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。 参考文献似然函数如何理解似然函数?对似然函数的理解最大似然估计总结笔记最大似然估计","categories":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/categories/统计学习/"}],"tags":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/tags/统计学习/"},{"name":"概率论","slug":"概率论","permalink":"http://sthsf.github.io/tags/概率论/"}]},{"title":"ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists.","slug":"ValueError: kernel already exists","date":"2017-06-18T02:00:00.000Z","updated":"2017-08-31T05:40:31.000Z","comments":true,"path":"2017/06/18/ValueError: kernel already exists/","link":"","permalink":"http://sthsf.github.io/2017/06/18/ValueError: kernel already exists/","excerpt":"","text":"写在前面最近在学习使用tensorflow构建language model，遇到关于模型重用的问题，我将模型的训练和预测放在同一个文件中时出现的问题,提示lstm_cell kernal已经存在. 错误提示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606148 Traceback (most recent call last): 49 File \"anna_writer.py\", line 274, in &lt;module&gt; 50 samp = generate_samples(checkpoint, 20000, prime=\"The \") 51 File \"anna_writer.py\", line 234, in generate_samples 52 conf.lstm_size, conf.keep_prob, conf.grad_clip, False) 53 File \"anna_writer.py\", line 74, in __init__ 54 self.add_lstm_cell() 55 File \"anna_writer.py\", line 110, in add_lstm_cell 56 initial_state=self.initial_state) 57 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 574, ii n dynamic_rnn 58 dtype=dtype) 59 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 737, ii n _dynamic_rnn_loop 60 swap_memory=swap_memory) 61 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2770, in while_loop 62 result = context.BuildLoop(cond, body, loop_vars, shape_invariants) 63 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2599, in BuildLoop 64 pred, body, original_loop_vars, loop_vars, shape_invariants) 65 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2549, in _BuildLoop 66 body_result = body(*packed_vars_for_body) 67 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 722, ii n _time_step 68 (output, new_state) = call_cell() 69 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 708, ii n &lt;lambda&gt; 70 call_cell = lambda: cell(input_t, state) 71 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", ll ine 180, in __call__ 72 return super(RNNCell, self).__call__(inputs, state) 73 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 444 1, in __call__ 74 outputs = self.call(inputs, *args, **kwargs) 75 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 916, in call 76 cur_inp, new_state = cell(cur_inp, cur_state) 77 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 752, in __call__ 78 output, new_state = self._cell(inputs, state, scope) 79 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 180, in __call__ 80 return super(RNNCell, self).__call__(inputs, state) 81 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 44 1, in __call__ 82 outputs = self.call(inputs, *args, **kwargs) 83 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 383, in call 84 concat = _linear([inputs, h], 4 * self._num_units, True) 85 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 1017, in _linear 86 initializer=kernel_initializer) 87 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable 88 use_resource=use_resource, custom_getter=custom_getter) 89 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable 90 use_resource=use_resource, custom_getter=custom_getter) 91 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable 92 validate_shape=validate_shape, use_resource=use_resource) 93 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1405, in wrapped_custom_getter 94 *args, **kwargs) 95 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 183, in _rnn_get_variable 96 variable = getter(*args, **kwargs) 97 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 183, in _rnn_get_variable 98 variable = getter(*args, **kwargs) 99 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter 100 use_resource=use_resource) 101 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 664, in _get_single_variable 102 name, \"\".join(traceback.format_list(tb)))) 103 ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:104 105 File \"anna_writer.py\", line 110, in add_lstm_cell 106 initial_state=self.initial_state) 107 File \"anna_writer.py\", line 74, in __init__ 108 self.add_lstm_cell() 109 File \"anna_writer.py\", line 172, in train 110 conf.grad_clip, is_training=True) 解决方法这个问题困扰了我两天，始终找不到解决方案，当我的训练模型和预测模型分开运行时程序没有报错，但是两个程序放在一起运行时就会出现问题，网上搜索的结果大都是关于共享权重的问题，错误提示是:1ValueError: Variable hello/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope?这个error跟我的错误还是有一定的区别的。这些问题主要原因在于使用多层lstm_cell或者双向lstm的时候忽略了定义变量的variable_scope，导致lstm_cell的作用域不一样，但是程序加载的时候并不知道，所以当声明的cell不是同一个的时候，需要用1with tf.variable_scope(name):来定义不同的作用范围就可以了，具体还要根据实际情况。而我的问题好像网上还没有这样的解释，我仔细看错误的提示，分析我的代码，当train和predict放在一起的时候，会调用两次class language_model：这时候就会出现系统里应该存在两个不同的lstm_cell模型，但是系统无法辨别出来，所以会提示kernel already exists，而不是weights already exists。而tensorflow有一个reset_default_graph()函数，我对python多线程不是很清楚，贴下源码，123456789def reset_default_graph(): \"\"\"Clears the default graph stack and resets the global default graph. NOTE: The default graph is a property of the current thread. This function applies only to the current thread. Calling this function while a `tf.Session` or `tf.InteractiveSession` is active will result in undefined behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects after calling this function will result in undefined behavior. \"\"\"然后在我定义的language_model类中添加这个函数之后之前的问题就解决了。12345678910111213141516171819202122232425262728293031import tensorflow as tfclass language_model: def __init__(self, num_classes, batch_size=100, seq_length=50, learning_rate=0.01, num_layers=5, hidden_units=128, keep_prob=0.8, grad_clip=5, is_training=True): # 模型的训练和预测放在同一个文件下时如果没有这个函数会报错。 tf.reset_default_graph() self.learning_rate = learning_rate self.num_layers = num_layers self.hidden_units = hidden_units self.is_training = is_training self.keep_prob = keep_prob self.grad_clip = grad_clip self.num_classes = num_classes if self.is_training: self.batch_size = batch_size self.seq_length = seq_length else: self.batch_size = 1 self.seq_length = 1 with tf.name_scope('add_input_layer'): self.add_input_layer() with tf.variable_scope('lstm_cell'): self.add_multi_cells() with tf.name_scope('build_output'): self.build_output() with tf.name_scope('cost'): self.compute_cost() with tf.name_scope('train_op'): self.train_op()题外话 ：tensorflow1.2版本之后，定义多层lstm(MultiRNNCell)与原来的版本改变比较大，可以看考PTB tutorials—Stacking multiple LSTMs.文中涉及到的代码见：github–anna 参考文献1、tensorflow1.x版本rnn生成cell 报错解决方案2、ValueError: Attempt to reuse RNNCell3、How to reuse weights in MultiRNNCell?4、ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/categories/Tensorflow/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Deeplearning","slug":"Deeplearning","permalink":"http://sthsf.github.io/tags/Deeplearning/"},{"name":"ValueError","slug":"ValueError","permalink":"http://sthsf.github.io/tags/ValueError/"}]},{"title":"Errors when buildding blog with hexo","slug":"Errors when buildding blog with hexo","date":"2017-03-18T02:51:24.000Z","updated":"2017-09-04T02:33:52.000Z","comments":true,"path":"2017/03/18/Errors when buildding blog with hexo/","link":"","permalink":"http://sthsf.github.io/2017/03/18/Errors when buildding blog with hexo/","excerpt":"","text":"Hexo 安装提交过程中错误以及解决方法 1、错误提示：12ERROR Process failed: _posts/Tensorflow中dynamic-rnn和row-rnn的区别.mdYAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at line 9, column 1:错误原因：原因是我在makrdown文件中的site那块添加categories的时候，后面的冒号使用的是中文状态下的输入。解决方法：中文冒号改写成英文冒号1234567891011---title: Tensorflow中dynamic_rnn和row_rnn的区别date: 2017-09-04 08:58:38catalog: truetags:- Tensorflow- Tensorflow基础知识categories:- Tensorflow基础知识---注意:注意中英文标点符号的问题，还有就是拼写错误，冒号后面的空格等等格式错误，有的时候不注意很容易写错。 2、错误提示12fatal: unable to access 'https://github.com/STHSF/sthsf.github.io/': Could not resolve host: github.comFATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html错误原因：可能是网络原因，导致无法连接到github的服务器。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://sthsf.github.io/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://sthsf.github.io/tags/Hexo/"},{"name":"Blog","slug":"Blog","permalink":"http://sthsf.github.io/tags/Blog/"}]}]}