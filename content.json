{"meta":{"title":"Yu Li's personal blog","subtitle":null,"description":null,"author":"Yu Li","url":"http://sthsf.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2017-10-20T00:52:34.329Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"/404.html","permalink":"http://sthsf.github.io//404.html","excerpt":"","text":""},{"title":"Categories","date":"2017-10-20T00:52:34.352Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"categories/index.html","permalink":"http://sthsf.github.io/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2019-03-14T01:42:39.833Z","updated":"2019-03-14T01:42:39.000Z","comments":true,"path":"about/index.html","permalink":"http://sthsf.github.io/about/index.html","excerpt":"","text":"李煜I am a self-motivated postgraduate student who is strongly committed to research.A wandering machine learning researcher, bouncing between groups.I want to understand things clearly, and explain them well.If you are interested in my CV, Don’t hesitate to contact me! Research ExperienceCapacitance tomography technology application and development of the research, 2014.11-2016.05Marine environment parameters more rapid detection and the key techniques of processing based on GIS, 2013.09-2016.05长江口附近海岸生态修复及生物资源利用技术及示范, 2013.09-2016.05海洋信息服务产业科技应用与产业发展研究, 2013.11-2013.12 Integrated Skills熟悉Python, Scala, Matlab, C++理解深度学习,熟悉Tensorflow、Keras等深度学习框架, 使用Tensorflow和Keras解决过实际问题了解知识图谱相关理论（实体消歧、语义搜索)了解Spark, Hadoop等大数据挖掘技术,熟悉linux编程环境。熟悉HBase、redis等主流的no sql数据库理解分类聚类回归问题, 熟悉SVM, NMF等机器学习算法, 熟悉自然语言理解。 Interested AreaAI, Machine Learning, Deeplearning, NLP… Open Source1、Keyword/Phase/Summary Extract Based On TextRank基于TextRank的关键词、短语、摘要提取程序，代码使用Scala编写。2、Text Classification based on word2vecText Classification Based on Word2Vec and TextRank3、Chinese Word Segmentation基于bidirectional_lstm的中文分词系统4、Text Generation基于lstm的languag model5、Sentiment Analysis Honors &amp; Awards国家二等奖, 第十二届“中关村青联杯”全国研究生数学建模竞赛国家三等奖, 第十一届“华为杯”全国研究生数学建模竞赛华东赛区三等奖, 第一届全国高校物联网应用创新大赛三等奖, 淮安市创业计划大赛"},{"title":"Books","date":"2017-10-20T00:52:34.351Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"books/index.html","permalink":"http://sthsf.github.io/books/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-10-20T00:52:34.355Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"tags/index.html","permalink":"http://sthsf.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repository","date":"2017-10-20T00:52:34.354Z","updated":"2017-08-15T18:06:22.000Z","comments":false,"path":"repository/index.html","permalink":"http://sthsf.github.io/repository/index.html","excerpt":"","text":""},{"title":"Links","date":"2017-10-20T00:52:34.353Z","updated":"2017-08-15T18:06:22.000Z","comments":true,"path":"links/index.html","permalink":"http://sthsf.github.io/links/index.html","excerpt":"","text":""}],"posts":[{"title":"Tensorflow中的共享变量机制","slug":"Tensorflow中易混淆的函数","date":"2017-09-14T02:14:56.000Z","updated":"2017-09-15T02:19:19.000Z","comments":true,"path":"2017/09/14/Tensorflow中易混淆的函数/","link":"","permalink":"http://sthsf.github.io/2017/09/14/Tensorflow中易混淆的函数/","excerpt":"","text":"写在前面本文总结了Tensorflow使用过程中易混淆的一些接口，比如get_variable()和Variable()。name_scope()和variable_scope()等。 tf.get_varibale()/ tf.Variable()tensorflow中关于variable的op有tf.get_variable()和tf.Variable两个. tf.get_variable()123456789101112tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None) tf.Variable()12345678910tf.Variable(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None) tf.get_variable()/tf.Variable()的区别先看下面的两个例子：12345import tensorflow as tf a1 = tf.Variable(0, name=\"a1\")a2 = tf.Variable(1, name='a1')print(a1.name)print(a2.name)输出结果12a1:0a1_1:012345import tensorflow as tfa1 = tf.get_variable(\"a1\", 0)a2 = tf.get_variable('a1', 0)print(a1.name)print(a2.name)输出结果：1ValueError: Variable a1 already exists, disallowed. Did you mean to set reuse=True in VarScope?使用tf.Varibale()定义变量的时候，如果检测到命名冲突，系统会自动解决，但是使用tf.get_varibale()时，系统不会解决冲突，并且会报错。所以如果需要共享变量则需要使用tf.get_variable()。在其他情况下两者的用法基本一样。我们再来看一段代码:12345678910import tensorflow as tfwith tf.variable_scope(\"scope1\"): w1 = tf.get_variable(\"w1\", shape=[]) w2 = tf.Variable(0.0, name=\"w2\")with tf.variable_scope(\"scope1\", reuse=True): w1_p = tf.get_variable(\"w1\", shape=[]) w2_p = tf.Variable(1.0, name=\"w2\") print(w1 is w1_p, w2 is w2_p)输出结果：1(True, False)从输出结果可以看出，对于get_variable()，来说，如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的。而tf.Variable()每次都在创建新对象。这里没有太多的提到共享变量的问题， tf.name_scope()/ tf.variable_scope() 参考文献tensorflow学习笔记（二十三）：variable与get_variable共享变量","categories":[{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/categories/Tensorflow基础知识/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/tags/Tensorflow基础知识/"}]},{"title":"Tensorflow中dynamic_rnn和row_rnn的区别","slug":"Tensorflow中dynamic-rnn和row-rnn的区别","date":"2017-09-04T00:58:38.000Z","updated":"2017-09-14T01:28:17.000Z","comments":true,"path":"2017/09/04/Tensorflow中dynamic-rnn和row-rnn的区别/","link":"","permalink":"http://sthsf.github.io/2017/09/04/Tensorflow中dynamic-rnn和row-rnn的区别/","excerpt":"","text":"写在前面 参考文献tensor flow dynamic_rnn 与rnn有啥区别？有大神能详细讲讲tensorflow中的raw_rnn这个函数么？","categories":[{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/categories/Tensorflow基础知识/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/tags/Tensorflow基础知识/"}]},{"title":"Tensorflow基础知识---Bidirectional_RNN","slug":"Tensorflow基础知识-bidirectional-rnn","date":"2017-08-31T02:17:27.000Z","updated":"2017-09-14T02:22:59.000Z","comments":true,"path":"2017/08/31/Tensorflow基础知识-bidirectional-rnn/","link":"","permalink":"http://sthsf.github.io/2017/08/31/Tensorflow基础知识-bidirectional-rnn/","excerpt":"","text":"写在前面最近在做一些自然语言处理demo的时候遇到了双向RNN，里面的bidirectional_dynamic_rnn和static_bidirectional_rnn还是值得理解下的，故记录下自己的学习心得。 双向RNNs双向RNNs模型是RNN的扩展模型，RNN模型在处理序列模型的学习上主要是依靠上文的信息，双向RNNs模型认为模型的输出不仅仅依靠序列前面的元素，后面的元素对输出也有影响。比如说，想要预测序列中的一个缺失值，我们不仅仅要考虑该缺失值前面的元素，而且要考虑他后面的元素。简单点来将两个RNN堆叠在一起，分别从两个方向计算序列的output和state，而最终的输出则根据两个RNNs的隐藏状态计算，如下图所示。Figure 1: A bidirectional RNN model在每一个时间节点(xt)(x_t)(x​t​​)，这个网络有两层神经元，一层从左向右传播，另一层从右向左传播。为了保证任何时刻t都有两层隐层，这个网络需要消耗两倍的存储量来存储权重和偏置等参数。最终的分类结果是由两层RNN隐层组合来产生最终的结果。公式1和2表示双向RNN隐层的数学含义。在这两个关系中唯一不同点是循环的方向不一样。公式3展示了通过总结过去和未来词的表示，使用类别的关系来预测下一个词预测。双向循环神经网络的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络，而且这两个都连接着一个输出层，这个结构提供给输出层输入序列中每个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层(w1,w3)，隐含层到隐含层自己(w2, w5),向前和向后隐含层到输出层(w4, w6)。值得注意的是：向后和向前隐含层之间没有信息流，是独立计算的，只是最后输出的时候把二者的状态向量结合起来，这保证了展开图是非循环的。Figure 2: Bidirectional RNN model unrolling Tensorflow中实现双向RNNs tf.contrib.rnn.bidirectional_dynamic_rnn()在tensorflow中已经提供了双向RNNs的接口，使用tf.contrib.rnn.bidirectional_dynamic_rnn()这个函数，就可以很方便的构建双向RNN网络。首先看下接口的一些参数1234567891011121314bidirectional_dynamic_rnn( cell_fw, # 前向 rnn cell cell_bw, # 反向 rnn cell inputs, # 输入序列. sequence_length=None, # 输入序列的实际长度（可选，默认为输入序列的最大长度） initial_state_fw=None, # 前向rnn_cell的初始状态（可选） initial_state_bw=None, # 反向rnn_cell的初始状态（可选） dtype=None, # 初始化和输出的数据类型（可选） parallel_iterations=None, swap_memory=False, time_major=False, # 决定了输入输出tensor的格式：如果为true, 向量的形状必须为 `[max_time, batch_size, depth]`. # 如果为false, tensor的形状必须为`[batch_size, max_time, depth]`. 与dynamic_rnn中的time_major类似。 scope=None)函数的返回值：一个（outputs, outputs_state）的一个元祖。其中，outputs=(outputs_fw, outputs_bw),是一个包含前向cell输出tensor和后向tensor输出tensor组成的元祖。若time_major=false，则两个tensor的shape为[batch_size, max_time, depth]，应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。最终的outputs需要使用tf.concat(outputs, 2)将两者合并起来。outputs_state = (outputs_state_fw， output_state_bw),包含了前向和后向最后的隐藏状态的组成的元祖。outputs_state_fw和output_state_bw的类型都是LSTMStateTuple。LSTMStateTuple由(c, h)组成，分别代表memory cell和hidden statecell_fw和cell_bw的定义是完全一样的，如果两个cell都定义成LSTM就变成说了双向LSTM了。12345import tensorflow as tf # 正向传播的rnn_cell单元，这里使用的是LSRTMCellcell_fw_lstm = tf.nn.rnn_cell.LSTMCell('num_units')# 反向传播的rnn_cell单元，与正向传播的rnn单元相同cell_bw_lstm = tf.nn.rnn_cell.LSTMCell('num_units')在bidirectional_dynamic_rnn函数内部，会通过array_ops.reverse_sequence函数将输入序列逆序排列，使其达到反向传播的效果。在实现的时候，我们只需要将定义好的两个cell作为参数传入就可以了：123(outputs, outputs_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw_lstm, cell_bw_lstm, inputs_embedded)# inputs_embedded为输入的tensor，[batch_szie, max_time, depth]。batch_size为模型当中batch的大小.# 应用在文本中时，max_time可以为句子的长度（一般以最长的句子为准，短句需要做padding），depth为输入句子词向量的维度。最终的输出outputs = tf.concat((outputs_fw, outputs_bw), 2)或者直接是outputs = tf.concat(outputs, 2)如果还需要用到最后的输出状态，则需要对（outputs_state_fw， output_state_bw）处理:1234final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), 1)final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), 1)outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c, h=final_state_h) 双向LSRTM的实现过如下：123456789101112131415161718192021import tensorflow as tf vocab_size = 1000embedding_size = 50batch_size =100max_time = 10hidden_units = 10inputs = tf.placeholder(shape=(batch_size, max_time), dtype=tf.int32, name='inputs')embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), dtype=tf.float32)inputs_embeded = tf.nn.embedding_lookup(embedding, inputs)lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)((outputs_fw, outputs_bw), (outputs_state_fw, outputs_state_bw)) = tf.nn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, inputs_embeded, sequence_length=max_time)outputs = tf.concat((outputs_fw, outputs_bw), 2)final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), 1)final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), 1)outputs_final_state = tf.contrib.rnn.LSTMStateTuple(c=final_state_c, h=final_state_h) tf.contrib.rnn.static_bidirectional_rnn() 多层双向RNNs图三展示了一个从较低层传播到下层的多层双向RNN。如图所示，在网络结构中，第t个时间里每一个中间神经元接受到前一个时间（同样的RNN层）传递过来的一组参数，以及之前RNN层传递过来的两组参数。这两组参数一个是从左到右的RNN输入，另一个是从右到左的RNN输入。Figure 3: Multi-Bidirectional RNN model为了构建一个L层的RNN，上述的关系将会参照公式4和公式5所修改，其中每一个中间神经元（第i层）的输入是RNN网络中同样的t时刻第i-1层的输出。其中输出，在每一个时刻值为通过所有隐层的输入参数传播的结果（如公式6所示）。 参考文献tensorflow.nn.bidirectional_dynamic_rnn()函数的用法深度学习与自然语言处理(7)_斯坦福cs224d 语言模型，RNN，LSTM与GRUtensorflow学习笔记(三十九):双向rnn","categories":[{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/categories/Tensorflow基础知识/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Tensorflow基础知识","slug":"Tensorflow基础知识","permalink":"http://sthsf.github.io/tags/Tensorflow基础知识/"}]},{"title":"极大似然估计学习总结","slug":"test","date":"2017-08-24T02:09:46.000Z","updated":"2018-02-06T02:03:33.141Z","comments":true,"path":"2017/08/24/test/","link":"","permalink":"http://sthsf.github.io/2017/08/24/test/","excerpt":"","text":"写在前面极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这也样本分布的参数，把可能性最大的那个参数作为真实参数估计。 极大似然估计的原理给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。 参考文献似然函数如何理解似然函数?对似然函数的理解最大似然估计总结笔记最大似然估计","categories":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/categories/统计学习/"}],"tags":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/tags/统计学习/"},{"name":"概率论","slug":"概率论","permalink":"http://sthsf.github.io/tags/概率论/"}]},{"title":"极大似然估计学习总结","slug":"极大似然估计学习总结","date":"2017-08-24T02:09:46.000Z","updated":"2017-08-24T02:32:27.000Z","comments":true,"path":"2017/08/24/极大似然估计学习总结/","link":"","permalink":"http://sthsf.github.io/2017/08/24/极大似然估计学习总结/","excerpt":"","text":"写在前面极大似然估计是一种统计方法，他是用来求一个样本集的相关概率密度函数的参数的，即在已知试验结果（样本）的情况下，用来估计满足这也样本分布的参数，把可能性最大的那个参数作为真实参数估计。 极大似然估计的原理给定一个概率分布#(D)#，假定其概率密度函数（连续分布）或者概率聚集函数（离散分布）为#(f_D)#,以及一个分布参数，我们可以从这个分布中抽出一个具有#(n)#个值的采样。 参考文献似然函数如何理解似然函数?对似然函数的理解最大似然估计总结笔记最大似然估计","categories":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/categories/统计学习/"}],"tags":[{"name":"统计学习","slug":"统计学习","permalink":"http://sthsf.github.io/tags/统计学习/"},{"name":"概率论","slug":"概率论","permalink":"http://sthsf.github.io/tags/概率论/"}]},{"title":"ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists.","slug":"ValueError: kernel already exists","date":"2017-06-18T02:00:00.000Z","updated":"2017-08-31T05:40:31.000Z","comments":true,"path":"2017/06/18/ValueError: kernel already exists/","link":"","permalink":"http://sthsf.github.io/2017/06/18/ValueError: kernel already exists/","excerpt":"","text":"写在前面最近在学习使用tensorflow构建language model，遇到关于模型重用的问题，我将模型的训练和预测放在同一个文件中时出现的问题,提示lstm_cell kernal已经存在. 错误提示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606148 Traceback (most recent call last): 49 File \"anna_writer.py\", line 274, in &lt;module&gt; 50 samp = generate_samples(checkpoint, 20000, prime=\"The \") 51 File \"anna_writer.py\", line 234, in generate_samples 52 conf.lstm_size, conf.keep_prob, conf.grad_clip, False) 53 File \"anna_writer.py\", line 74, in __init__ 54 self.add_lstm_cell() 55 File \"anna_writer.py\", line 110, in add_lstm_cell 56 initial_state=self.initial_state) 57 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 574, ii n dynamic_rnn 58 dtype=dtype) 59 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 737, ii n _dynamic_rnn_loop 60 swap_memory=swap_memory) 61 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2770, in while_loop 62 result = context.BuildLoop(cond, body, loop_vars, shape_invariants) 63 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2599, in BuildLoop 64 pred, body, original_loop_vars, loop_vars, shape_invariants) 65 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\"\" , line 2549, in _BuildLoop 66 body_result = body(*packed_vars_for_body) 67 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 722, ii n _time_step 68 (output, new_state) = call_cell() 69 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 708, ii n &lt;lambda&gt; 70 call_cell = lambda: cell(input_t, state) 71 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", ll ine 180, in __call__ 72 return super(RNNCell, self).__call__(inputs, state) 73 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 444 1, in __call__ 74 outputs = self.call(inputs, *args, **kwargs) 75 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 916, in call 76 cur_inp, new_state = cell(cur_inp, cur_state) 77 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 752, in __call__ 78 output, new_state = self._cell(inputs, state, scope) 79 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 180, in __call__ 80 return super(RNNCell, self).__call__(inputs, state) 81 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 44 1, in __call__ 82 outputs = self.call(inputs, *args, **kwargs) 83 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 383, in call 84 concat = _linear([inputs, h], 4 * self._num_units, True) 85 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 1017, in _linear 86 initializer=kernel_initializer) 87 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable 88 use_resource=use_resource, custom_getter=custom_getter) 89 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable 90 use_resource=use_resource, custom_getter=custom_getter) 91 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable 92 validate_shape=validate_shape, use_resource=use_resource) 93 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1405, in wrapped_custom_getter 94 *args, **kwargs) 95 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 183, in _rnn_get_variable 96 variable = getter(*args, **kwargs) 97 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", l ine 183, in _rnn_get_variable 98 variable = getter(*args, **kwargs) 99 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter 100 use_resource=use_resource) 101 File \"/home/tf1.0/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 664, in _get_single_variable 102 name, \"\".join(traceback.format_list(tb)))) 103 ValueError: Variable lstm_cell/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:104 105 File \"anna_writer.py\", line 110, in add_lstm_cell 106 initial_state=self.initial_state) 107 File \"anna_writer.py\", line 74, in __init__ 108 self.add_lstm_cell() 109 File \"anna_writer.py\", line 172, in train 110 conf.grad_clip, is_training=True) 解决方法这个问题困扰了我两天，始终找不到解决方案，当我的训练模型和预测模型分开运行时程序没有报错，但是两个程序放在一起运行时就会出现问题，网上搜索的结果大都是关于共享权重的问题，错误提示是:1ValueError: Variable hello/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope?这个error跟我的错误还是有一定的区别的。这些问题主要原因在于使用多层lstm_cell或者双向lstm的时候忽略了定义变量的variable_scope，导致lstm_cell的作用域不一样，但是程序加载的时候并不知道，所以当声明的cell不是同一个的时候，需要用1with tf.variable_scope(name):来定义不同的作用范围就可以了，具体还要根据实际情况。而我的问题好像网上还没有这样的解释，我仔细看错误的提示，分析我的代码，当train和predict放在一起的时候，会调用两次class language_model：这时候就会出现系统里应该存在两个不同的lstm_cell模型，但是系统无法辨别出来，所以会提示kernel already exists，而不是weights already exists。而tensorflow有一个reset_default_graph()函数，我对python多线程不是很清楚，贴下源码，123456789def reset_default_graph(): \"\"\"Clears the default graph stack and resets the global default graph. NOTE: The default graph is a property of the current thread. This function applies only to the current thread. Calling this function while a `tf.Session` or `tf.InteractiveSession` is active will result in undefined behavior. Using any previously created `tf.Operation` or `tf.Tensor` objects after calling this function will result in undefined behavior. \"\"\"然后在我定义的language_model类中添加这个函数之后之前的问题就解决了。12345678910111213141516171819202122232425262728293031import tensorflow as tfclass language_model: def __init__(self, num_classes, batch_size=100, seq_length=50, learning_rate=0.01, num_layers=5, hidden_units=128, keep_prob=0.8, grad_clip=5, is_training=True): # 模型的训练和预测放在同一个文件下时如果没有这个函数会报错。 tf.reset_default_graph() self.learning_rate = learning_rate self.num_layers = num_layers self.hidden_units = hidden_units self.is_training = is_training self.keep_prob = keep_prob self.grad_clip = grad_clip self.num_classes = num_classes if self.is_training: self.batch_size = batch_size self.seq_length = seq_length else: self.batch_size = 1 self.seq_length = 1 with tf.name_scope('add_input_layer'): self.add_input_layer() with tf.variable_scope('lstm_cell'): self.add_multi_cells() with tf.name_scope('build_output'): self.build_output() with tf.name_scope('cost'): self.compute_cost() with tf.name_scope('train_op'): self.train_op()题外话 ：tensorflow1.2版本之后，定义多层lstm(MultiRNNCell)与原来的版本改变比较大，可以看考PTB tutorials—Stacking multiple LSTMs.文中涉及到的代码见：github–anna 参考文献1、tensorflow1.x版本rnn生成cell 报错解决方案2、ValueError: Attempt to reuse RNNCell3、How to reuse weights in MultiRNNCell?4、ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/categories/Tensorflow/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://sthsf.github.io/tags/Tensorflow/"},{"name":"Deeplearning","slug":"Deeplearning","permalink":"http://sthsf.github.io/tags/Deeplearning/"},{"name":"ValueError","slug":"ValueError","permalink":"http://sthsf.github.io/tags/ValueError/"}]},{"title":"Errors when buildding blog with hexo","slug":"Errors when buildding blog with hexo","date":"2017-03-18T02:51:24.000Z","updated":"2017-09-04T02:33:52.000Z","comments":true,"path":"2017/03/18/Errors when buildding blog with hexo/","link":"","permalink":"http://sthsf.github.io/2017/03/18/Errors when buildding blog with hexo/","excerpt":"","text":"Hexo 安装提交过程中错误以及解决方法 1、错误提示：12ERROR Process failed: _posts/Tensorflow中dynamic-rnn和row-rnn的区别.mdYAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at line 9, column 1:错误原因：原因是我在makrdown文件中的site那块添加categories的时候，后面的冒号使用的是中文状态下的输入。解决方法：中文冒号改写成英文冒号1234567891011---title: Tensorflow中dynamic_rnn和row_rnn的区别date: 2017-09-04 08:58:38catalog: truetags:- Tensorflow- Tensorflow基础知识categories:- Tensorflow基础知识---注意:注意中英文标点符号的问题，还有就是拼写错误，冒号后面的空格等等格式错误，有的时候不注意很容易写错。 2、错误提示12fatal: unable to access 'https://github.com/STHSF/sthsf.github.io/': Could not resolve host: github.comFATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html错误原因：可能是网络原因，导致无法连接到github的服务器。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://sthsf.github.io/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://sthsf.github.io/tags/Hexo/"},{"name":"Blog","slug":"Blog","permalink":"http://sthsf.github.io/tags/Blog/"}]}]}